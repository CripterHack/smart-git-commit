#!/usr/bin/env python3
"""
Smart Git Commit Workflow with Ollama Integration

An advanced git commit workflow tool that uses Ollama with GPU acceleration
to intelligently analyze and group changes, generate meaningful commit messages,
and adapt to different tech stacks automatically.
"""

import os
import sys
import re
import json
import time
import argparse
import logging
import threading
import subprocess
import urllib.parse
from urllib.parse import urlparse
import socket
import http.client
import platform
import ctypes
import inspect
import unicodedata
import io
import datetime
import difflib
import shutil
import tempfile
import random
import enum
import textwrap
import requests
import concurrent.futures
from typing import (
    Dict, List, Tuple, Optional, Union, Set, Any, 
    Callable, Generator, TypeVar, Generic, Iterable
)
from enum import Enum
from dataclasses import dataclass, field
from pathlib import Path
from collections import defaultdict

from .colors import Colors, supports_color

# Access get_version from the package
from . import get_version

# Import CLI wizard (with fallback for backward compatibility)
try:
    from .cli_wizard import run_cli_welcome_wizard
except ImportError:
    # Define a fallback if the import fails
    def run_cli_welcome_wizard(config, use_color):
        logger.warning("CLI wizard module not available")
        config.set("welcome_completed", True)
        try:
            config.save()
        except Exception as e:
            logger.warning(f"Failed to save config: {e}")

# Set up logging
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

# Security-related enums and classes
class SecuritySeverity(enum.Enum):
    """Severity levels for security vulnerabilities."""
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"
    CRITICAL = "critical"

@dataclass
class SecurityVulnerability:
    """Represents a security vulnerability detected in code or files."""
    description: str
    severity: SecuritySeverity
    file_path: Optional[str] = None
    line_number: Optional[int] = None
    recommendation: Optional[str] = None

@dataclass
class SecurityScanResult:
    """Result of a security scan on a file or content."""
    is_vulnerable: bool
    vulnerabilities: List[SecurityVulnerability] = field(default_factory=list)
    scan_timestamp: datetime.datetime = field(default_factory=datetime.datetime.now)

# Change state enum for tracking file changes
class ChangeState(enum.Enum):
    """States of a file change in version control."""
    ADDED = "added"
    MODIFIED = "modified"
    DELETED = "deleted"
    RENAMED = "renamed"
    UNTRACKED = "untracked"
    IGNORED = "ignored"

# Set default encoding to UTF-8
if sys.stdout.encoding != 'utf-8':
    if hasattr(sys.stdout, 'reconfigure'):
        sys.stdout.reconfigure(encoding='utf-8')
    elif hasattr(sys.stdout, 'detach'):
        import io
        sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

# Constants
DEFAULT_TIMEOUT = 60  # Default timeout in seconds for HTTP requests
SPINNER_CHARS = [
    ['â ‹', 'â ™', 'â ¹', 'â ¸', 'â ¼', 'â ´', 'â ¦', 'â §', 'â ‡', 'â '],
    ['â£¾', 'â£½', 'â£»', 'â¢¿', 'â¡¿', 'â£Ÿ', 'â£¯', 'â£·'],
    [' ', 'â–ƒ', 'â–„', 'â–…', 'â–†', 'â–‡', 'â–ˆ', 'â–‡', 'â–†', 'â–…', 'â–„', 'â–ƒ'],
    ['â–‰', 'â–Š', 'â–‹', 'â–Œ', 'â–', 'â–Ž', 'â–', 'â–Ž', 'â–', 'â–Œ', 'â–‹', 'â–Š', 'â–‰'],
    ['â––', 'â–˜', 'â–', 'â–—'],
    ['â–Œ', 'â–€', 'â–', 'â–„'],
]

# Colors class moved to colors.py
# supports_color function moved to colors.py

class SecurityScanner:
    """
    Security scanner to detect sensitive data in files that should not be committed.
    
    This class provides methods to scan file content and paths for:
    - Environment files and configuration with potential secrets
    - API keys, tokens, and credentials in code
    - Private keys and certificates
    - Temporary or draft files that shouldn't be committed
    - Database connection strings with passwords
    - Hardcoded IP addresses and internal URLs
    
    Usage:
        scanner = SecurityScanner()
        is_sensitive, reason = scanner.scan_file(filename, content)
        if is_sensitive:
            print(f"Warning: {filename} contains sensitive data: {reason}")
    """
    
    # Common sensitive file patterns
    SENSITIVE_FILES = [
        # Environment and config files with secrets
        r"\.env($|\..*$)",           # .env, .env.local, .env.development, etc.
        r".*\.config$",              # *.config files that might contain connection strings
        r"config\.php$",             # PHP config files
        r"settings\.json$",          # Settings files 
        r"credentials\..*$",         # Any credentials.* file
        r"\.htpasswd$",              # Apache password files
        r"\.netrc$",                 # .netrc files with credentials
        
        # Keys and certificates
        r".*\.pem$",                 # PEM certificates
        r".*\.key$",                 # Private keys
        r".*\.keystore$",            # Java keystores
        r".*\.jks$",                 # Java key stores
        r".*\.p12$",                 # PKCS#12 certificates
        r".*\.pfx$",                 # PFX certificates
        r".*\.crt$",                 # Certificates
        r"id_rsa$",                  # SSH private keys
        r"id_dsa$",                  # SSH DSA keys
        r"\.ssh/.*_id$",             # SSH keys
        
        # Database files
        r".*\.sqlite$",              # SQLite database
        r".*\.sqlite3$",             # SQLite3 database
        r".*\.db$",                  # Generic database files
        r".*\.mdb$",                 # Microsoft Access database
        
        # Backup and temporary files
        r".*~$",                     # Backup files ending with ~
        r".*\.bak$",                 # Backup files
        r".*\.swp$",                 # Vim swap files
        r".*\.tmp$",                 # Temporary files
        r".*\.temp$",                # Temporary files
        r"\.DS_Store$",              # macOS folder settings
        r"Thumbs\.db$",              # Windows thumbnail cache
        r"\.idea/",                  # JetBrains IDE files
        r"\.vscode/",                # VS Code settings (may contain paths)
        
        # Draft and internal files
        r"DRAFT.*\..*$",             # Files marked as DRAFT
        r"TODO\..*$",                # TODO files
        r"FIXME\..*$",               # FIXME files
        r".*-WIP\..*$",              # Work in progress files
        r".*\.log$",                 # Log files
    ]
    
    # Content patterns that might contain sensitive data
    SENSITIVE_CONTENT_PATTERNS = [
        # API Keys and tokens
        r"api[_\-]?key.*[=:]\s*['\"]([\w\d]{16,})['\"]",  # API keys
        r"secret.*[=:]\s*['\"]([\w\d]{16,})['\"]",        # Secret keys
        r"token.*[=:]\s*['\"]([\w\d]{16,})['\"]",         # Tokens
        r"password.*[=:]\s*['\"]([\w\d@$!%*?&]{8,})['\"]", # Passwords
        r"auth.*[=:]\s*['\"]([\w\d]{16,})['\"]",          # Auth tokens
        
        # Database connection strings
        r"jdbc:.*:\/\/.*:.*@.*",                          # JDBC connection strings
        r"mongodb(\+srv)?:\/\/.*:.*@.*",                  # MongoDB connection strings
        r"postgres:\/\/.*:.*@.*",                         # PostgreSQL connection strings
        r"mysql:\/\/.*:.*@.*",                            # MySQL connection strings
        r"sqlserver:\/\/.*:.*@.*",                        # SQL Server connection strings
        r"redis:\/\/.*:.*@.*",                            # Redis connection strings
        
        # AWS keys
        r"AKIA[0-9A-Z]{16}",                             # AWS Access Key IDs
        r"aws_secret_access_key.*[=:]\s*['\"]([\w\d/+]{40})['\"]", # AWS Secret Access Keys
        
        # Private keys in code
        r"-----BEGIN (\w+) PRIVATE KEY-----",             # Private keys
        r"-----BEGIN OPENSSH PRIVATE KEY-----",           # OpenSSH private keys
        
        # IP addresses (potentially internal)
        r"192\.168\.\d{1,3}\.\d{1,3}",                    # 192.168.x.x addresses
        r"10\.\d{1,3}\.\d{1,3}\.\d{1,3}",                 # 10.x.x.x addresses
        r"172\.(1[6-9]|2[0-9]|3[0-1])\.\d{1,3}\.\d{1,3}", # 172.16-31.x.x addresses
        
        # Auth and security related
        r"Bearer\s+[A-Za-z0-9-_=]+\.[A-Za-z0-9-_=]+\.?[A-Za-z0-9-_.+/=]*", # JWT tokens
        r"eyJ[a-zA-Z0-9]{10,}\.eyJ[a-zA-Z0-9]{10,}\.[a-zA-Z0-9_-]{10,}",   # JWT structure
        
        # SSH, FTP, and connection info
        r"ssh:\/\/.*:.*@.*",                              # SSH connection strings
        r"ftp:\/\/.*:.*@.*",                              # FTP with credentials
        r"sftp:\/\/.*:.*@.*",                             # SFTP with credentials
    ]
    
    def __init__(self):
        """Initialize the security scanner with compiled regex patterns."""
        self.file_patterns = [re.compile(pattern, re.IGNORECASE) for pattern in self.SENSITIVE_FILES]
        self.content_patterns = [re.compile(pattern) for pattern in self.SENSITIVE_CONTENT_PATTERNS]
    
    def scan_filename(self, filename: str) -> Tuple[bool, str]:
        """
        Scan a filename to check if it matches sensitive file patterns.
        
        Args:
            filename: Path to the file to scan
            
        Returns:
            Tuple of (is_sensitive, reason)
        """
        basename = os.path.basename(filename)
        for i, pattern in enumerate(self.file_patterns):
            if pattern.search(basename) or pattern.search(filename):
                return True, f"Filename matches sensitive pattern: {self.SENSITIVE_FILES[i]}"
        
        return False, ""
    
    def scan_content(self, content: str) -> Tuple[bool, str]:
        """
        Scan content for sensitive data patterns.
        
        Args:
            content: The content string to scan
            
        Returns:
            Tuple of (is_sensitive, reason)
        """
        # Skip empty content
        if not content or len(content) < 10:
            return False, ""
            
        # Limit content scan to first 100KB to avoid performance issues with large files
        content_to_scan = content[:100000]
        
        # Check for sensitive data patterns
        for pattern in self.content_patterns:
            matches = pattern.findall(content_to_scan)
            if matches:
                # Mask the pattern in the reason for security
                pattern_display = str(pattern.pattern)
                # Safe replacement that avoids backslash issues in f-strings
                if '(' in pattern_display:
                    pattern_display = pattern_display.split('(')[0] + '(...)'
                
                return True, f"Content contains potential sensitive data matching pattern: {pattern_display}"
                
        return False, ""
    
    def scan_file(self, filename: str, content: Optional[str] = None) -> Tuple[bool, str]:
        """
        Scan both filename and content (if provided) for sensitive data.
        
        Args:
            filename: Path to the file to scan
            content: Optional string content of the file (if already loaded)
            
        Returns:
            Tuple of (is_sensitive, reason)
        """
        # First check the filename
        is_sensitive, reason = self.scan_filename(filename)
        if is_sensitive:
            return True, reason
        
        # If content is provided, check that too
        if content:
            is_sensitive, reason = self.scan_content(content)
            if is_sensitive:
                return True, reason
        
        return False, ""


class Spinner:
    """Displays a spinner in the console to indicate progress."""

    def __init__(self, message="Loading...", spinner_type=1, delay=0.1, stream=sys.stdout, 
                 show_progress_bar=False, total=100, width=20):
        """
        Initialize the spinner.
        
        Args:
            message: Message to display next to the spinner
            spinner_type: Type of spinner animation (0-5)
            delay: Delay between spinner frames in seconds
            stream: Stream to write to (default: stdout)
            show_progress_bar: Whether to show a progress bar
            total: Total number of steps for progress bar
            width: Width of the progress bar in characters
        """
        self.message = message
        self.delay = delay
        self.spinner_chars = SPINNER_CHARS[spinner_type % len(SPINNER_CHARS)]
        self.stream = stream
        self.stop_running = threading.Event()
        self.spin_thread = None
        self.show_progress_bar = show_progress_bar
        self.total = total
        self.progress = 0
        self.width = width
        self.last_update = ""

    def _spin(self):
        """Spin the spinner."""
        while not self.stop_running.is_set():
            for char in self.spinner_chars:
                if self.stop_running.is_set():
                    break

                if self.show_progress_bar:
                    # Calculate progress bar
                    filled_width = int(self.width * (self.progress / self.total))
                    filled = "â–ˆ" * filled_width
                    empty = " " * (self.width - filled_width)
                    percent = int(100 * (self.progress / self.total))
                    
                    # Format the progress bar
                    progress_bar = f"[{filled}{empty}] {percent}%"
                    output = f"\r{char} {self.message} {progress_bar}"
                else:
                    output = f"\r{char} {self.message}"
                
                # Only update if output changed
                if output != self.last_update:
                    self.stream.write(output)
                    self.stream.flush()
                    self.last_update = output
                
                time.sleep(self.delay)

    def start(self):
        """Start the spinner."""
        self.stop_running.clear()
        self.spin_thread = threading.Thread(target=self._spin)
        self.spin_thread.daemon = True
        self.spin_thread.start()
        return self

    def stop(self):
        """Stop the spinner."""
        self.stop_running.set()
        if self.spin_thread is not None:
            self.spin_thread.join()
        self.stream.write('\r' + ' ' * (len(self.last_update) + 10) + '\r')
        self.stream.flush()

    def update(self, message, progress=None):
        """Update the spinner message and/or progress."""
        self.message = message
        if progress is not None and self.show_progress_bar:
            self.progress = min(progress, self.total)

    def __enter__(self):
        """Context manager enter method."""
        self.start()
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        """Context manager exit method."""
        self.stop()
        return False


def get_system_resources() -> Dict[str, Any]:
    """
    Get information about system resources such as CPU, memory, and GPU.
    
    Returns:
        Dict[str, Any]: Dictionary containing system resource information
    """
    resources = {
        "cpu_count": os.cpu_count() or 1,
        "threads_available": max(1, (os.cpu_count() or 2) - 1),  # Leave one CPU for system
        "memory_gb": 0.0,
        "free_memory": 0.0,
        "gpu_available": False,
        "gpu_names": [],
        "batch_size": 10  # Default batch size if we can't determine system resources
    }
    
    # Try to get memory info
    try:
        if platform.system() == "Linux":
            with open('/proc/meminfo', 'r') as f:
                for line in f:
                    if 'MemTotal' in line:
                        # Extract the memory size in KB and convert to GB
                        mem_kb = int(line.split()[1])
                        resources["memory_gb"] = round(mem_kb / 1024 / 1024, 1)
                        break
        elif platform.system() == "Darwin":  # macOS
            # Use subprocess to run vm_stat command
            try:
                vm_stat = subprocess.run(['vm_stat'], capture_output=True, text=True, check=True)
                page_size_line = subprocess.run(['sysctl', 'hw.pagesize'], capture_output=True, text=True, check=True)
                
                # Extract page size
                page_size = int(page_size_line.stdout.split(':')[1].strip())
                
                # Extract memory info from vm_stat output
                mem_dict = {}
                for line in vm_stat.stdout.splitlines()[1:]:
                    if ':' in line:
                        key, value = line.split(':')
                        key = key.strip()
                        value = int(value.strip().replace('.', ''))
                        mem_dict[key] = value
                
                # Calculate total memory
                total_pages = sum(mem_dict.get(k, 0) for k in ['Pages wired down', 'Pages active', 'Pages inactive', 'Pages free'])
                total_memory_bytes = total_pages * page_size
                resources["memory_gb"] = round(total_memory_bytes / 1024**3, 1)
            except (subprocess.SubprocessError, ValueError) as e:
                logger.debug(f"Error getting memory info on macOS: {e}")
        elif platform.system() == "Windows":
            # Use ctypes to access Windows API
            try:
                import ctypes
                kernel32 = ctypes.windll.kernel32
                
                # Call GlobalMemoryStatusEx
                class MEMORYSTATUSEX(ctypes.Structure):
                    _fields_ = [
                        ('dwLength', ctypes.c_ulong),
                        ('dwMemoryLoad', ctypes.c_ulong),
                        ('ullTotalPhys', ctypes.c_ulonglong),
                        ('ullAvailPhys', ctypes.c_ulonglong),
                        ('ullTotalPageFile', ctypes.c_ulonglong),
                        ('ullAvailPageFile', ctypes.c_ulonglong),
                        ('ullTotalVirtual', ctypes.c_ulonglong),
                        ('ullAvailVirtual', ctypes.c_ulonglong),
                        ('ullAvailExtendedVirtual', ctypes.c_ulonglong),
                    ]
                
                memory_status = MEMORYSTATUSEX()
                memory_status.dwLength = ctypes.sizeof(MEMORYSTATUSEX)
                kernel32.GlobalMemoryStatusEx(ctypes.byref(memory_status))
                
                # Convert bytes to GB
                resources["memory_gb"] = round(memory_status.ullTotalPhys / 1024**3, 1)
            except Exception as e:
                logger.debug(f"Error getting memory info on Windows: {e}")
    except Exception as e:
        logger.debug(f"Error getting memory info: {e}")
    
    # Try to detect GPU
    try:
        # Check for NVIDIA GPU with nvidia-smi
        try:
            nvidia_smi = subprocess.run(['nvidia-smi', '--query-gpu=name', '--format=csv,noheader'], 
                                       capture_output=True, text=True, timeout=2)
            if nvidia_smi.returncode == 0 and nvidia_smi.stdout.strip():
                resources["gpu_available"] = True
                resources["gpu_names"] = [gpu.strip() for gpu in nvidia_smi.stdout.splitlines() if gpu.strip()]
        except (subprocess.SubprocessError, FileNotFoundError):
            pass
        
        # Check for AMD GPU with rocm-smi on Linux
        if not resources["gpu_available"] and platform.system() == "Linux":
            try:
                rocm_smi = subprocess.run(['rocm-smi', '--showproductname'], capture_output=True, text=True, timeout=2)
                if rocm_smi.returncode == 0 and 'GPU' in rocm_smi.stdout:
                    resources["gpu_available"] = True
                    # Extract GPU names from rocm-smi output
                    for line in rocm_smi.stdout.splitlines():
                        if 'GPU' in line and ':' in line:
                            gpu_name = line.split(':', 1)[1].strip()
                            if gpu_name:
                                resources["gpu_names"].append(gpu_name)
            except (subprocess.SubprocessError, FileNotFoundError):
                pass
            
        # Check for Intel GPU on Linux
        if not resources["gpu_available"] and platform.system() == "Linux":
            try:
                # Look for Intel GPUs in lspci output
                lspci = subprocess.run(['lspci'], capture_output=True, text=True, timeout=2)
                if lspci.returncode == 0:
                    for line in lspci.stdout.splitlines():
                        if 'VGA' in line and ('Intel' in line or 'UHD' in line or 'Iris' in line):
                            resources["gpu_available"] = True
                            gpu_name = line.split(':', 1)[1].strip() if ':' in line else "Intel GPU"
                            resources["gpu_names"].append(gpu_name)
                            break
            except (subprocess.SubprocessError, FileNotFoundError):
                pass
                
        # Check for GPU on macOS
        if not resources["gpu_available"] and platform.system() == "Darwin":
            try:
                system_profiler = subprocess.run(['system_profiler', 'SPDisplaysDataType'], 
                                               capture_output=True, text=True, timeout=3)
                if system_profiler.returncode == 0:
                    # Parse system_profiler output to find GPU info
                    gpu_section = False
                    for line in system_profiler.stdout.splitlines():
                        if "Graphics/Displays:" in line:
                            gpu_section = True
                        elif gpu_section and ":" in line and "Chipset Model:" in line:
                            gpu_name = line.split(':', 1)[1].strip()
                            resources["gpu_available"] = True
                            resources["gpu_names"].append(gpu_name)
            except (subprocess.SubprocessError, FileNotFoundError):
                pass
                
        # Check for GPU on Windows
        if not resources["gpu_available"] and platform.system() == "Windows":
            try:
                wmic = subprocess.run(['wmic', 'path', 'win32_VideoController', 'get', 'name'], 
                                    capture_output=True, text=True, timeout=3)
                if wmic.returncode == 0:
                    for line in wmic.stdout.splitlines()[1:]:  # Skip header
                        if line.strip():
                            resources["gpu_available"] = True
                            resources["gpu_names"].append(line.strip())
            except (subprocess.SubprocessError, FileNotFoundError):
                pass
    except Exception as e:
        logger.debug(f"Error detecting GPU: {e}")
    
    # Calculate adaptive batch size based on CPU cores
    # More cores can handle larger batches, up to a reasonable limit
    resources["batch_size"] = max(5, min(20, (resources["cpu_count"] * 2)))
    logger.debug(f"Adaptive batch size set to: {resources['batch_size']}")
    
    return resources


class CommitType(Enum):
    """Types of commits following Conventional Commits specification and GitHub practices."""
    FEAT = "feat"           # New feature
    FIX = "fix"             # Bug fix
    DOCS = "docs"           # Documentation changes
    STYLE = "style"         # Code style/formatting changes (no code change)
    REFACTOR = "refactor"   # Code refactoring (no feature change)
    TEST = "test"           # Adding/fixing tests
    CHORE = "chore"         # Routine tasks, maintenance
    PERF = "perf"           # Performance improvements
    BUILD = "build"         # Build system changes
    CI = "ci"               # CI/CD changes
    REVERT = "revert"       # Revert a previous commit
    SECURITY = "security"   # Security fixes
    DEPS = "deps"           # Dependency updates
    I18N = "i18n"           # Internationalization/localization
    A11Y = "a11y"           # Accessibility improvements
    UI = "ui"               # UI/UX improvements
    HOTFIX = "hotfix"       # Critical fixes for production
    WIP = "wip"             # Work in progress


@dataclass
class GitChange:
    """Represents a modified or untracked file in git."""
    status: str  # M, A, D, R, ?? etc.
    filename: str
    content_diff: Optional[str] = None
    language: Optional[str] = None
    tech_stack: Optional[List[str]] = None
    importance: float = 1.0
    is_sensitive: bool = False
    sensitive_reason: str = ""
    
    @property
    def file_type(self) -> str:
        """Return the file type based on extension."""
        _, ext = os.path.splitext(self.filename)
        return ext.strip('.').lower() if ext else "unknown"
    
    @property
    def component(self) -> str:
        """Determine the component based on the file path."""
        parts = self.filename.split(os.path.sep)
        
        # Handle root-level files
        if len(parts) == 1:
            # Configuration files
            if parts[0].startswith(".git") or parts[0] == ".gitignore":
                return "git-config"
            if parts[0].startswith("README"):
                return "docs"
            if parts[0].startswith(".env") or parts[0].endswith(".env"):
                return "config"
            if parts[0] in ["package.json", "package-lock.json", "yarn.lock", "requirements.txt", 
                            "Pipfile", "Pipfile.lock", "go.mod", "go.sum", "Gemfile", "Gemfile.lock",
                            "composer.json", "composer.lock", "poetry.lock", "pyproject.toml"]:
                return "dependencies"
            if parts[0].startswith("Dockerfile") or parts[0].startswith("docker-compose"):
                return "docker"
            if parts[0] in [".travis.yml", ".github", "circle.yml", ".gitlab-ci.yml", "azure-pipelines.yml"]:
                return "ci"
            if parts[0].endswith(".py"):
                return "core"
            return "root"
        
        # Handle architecture-specific directory structures
        
        # Frontend frameworks structure
        fe_component_dirs = {
            # React/Vue/Angular common structure
            "components": "ui-components",
            "pages": "ui-pages",
            "routes": "routing",
            "store": "state",
            "redux": "state",
            "context": "state",
            "hooks": "hooks",
            "services": "api-client",
            "assets": "assets",
            "styles": "styles",
            "layouts": "ui-layouts",
            "utils": "utils",
            "constants": "constants",
            "locales": "i18n",
            "translations": "i18n"
        }
        
        # Backend frameworks structure
        be_component_dirs = {
            # Django/Flask/Express/Rails/Spring common structure
            "controllers": "controllers",
            "views": "views",
            "templates": "templates",
            "models": "data-models",
            "schemas": "data-models",
            "repositories": "data-access",
            "migrations": "db-migrations",
            "middleware": "middleware",
            "serializers": "serializers",
            "services": "services",
            "utils": "utils",
            "helpers": "utils",
            "config": "config",
            "settings": "config",
            "api": "api"
        }
        
        # Monorepo structure detection
        if parts[0] in ["packages", "apps", "services", "modules"]:
            if len(parts) > 2:
                # This is a monorepo with a structure like packages/package-name/src/...
                package_name = parts[1]
                if len(parts) > 3:
                    # Check if the third part is a recognized component
                    component_type = parts[2].lower()
                    if component_type in fe_component_dirs:
                        return f"{package_name}-{fe_component_dirs[component_type]}"
                    if component_type in be_component_dirs:
                        return f"{package_name}-{be_component_dirs[component_type]}"
                return f"{parts[0]}-{package_name}"
            return parts[0]
        
        # Handle special directories common in many tech stacks
        if parts[0] in ("src", "app", "lib", "internal"):
            # If there's a subdirectory, use that for more specificity
            if len(parts) > 2:
                component_type = parts[1].lower()
                if component_type in fe_component_dirs:
                    return f"{parts[0]}-{fe_component_dirs[component_type]}"
                if component_type in be_component_dirs:
                    return f"{parts[0]}-{be_component_dirs[component_type]}"
                return f"{parts[0]}-{parts[1]}"
            return parts[0]
            
        # Check for frontend components
        if parts[0].lower() in fe_component_dirs:
            return fe_component_dirs[parts[0].lower()]
            
        # Check for backend components
        if parts[0].lower() in be_component_dirs:
            return be_component_dirs[parts[0].lower()]
            
        # Handle common directory names across tech stacks
        common_dirs = {
            "docs": ["docs", "documentation", "wiki", "guides"],
            "tests": ["test", "tests", "spec", "specs", "__tests__", "cypress", "e2e"],
            "config": ["config", "configs", "conf", "settings", "env"],
            "scripts": ["scripts", "tools", "bin", "utilities"],
            "styles": ["css", "styles", "scss", "sass"],
            "api": ["api", "endpoints", "routes", "controllers"],
            "models": ["models", "entities", "schemas", "types"],
            "utils": ["utils", "helpers", "common", "shared"],
            "assets": ["assets", "static", "public", "resources", "images", "media"]
        }
        
        for category, dir_names in common_dirs.items():
            if parts[0].lower() in dir_names:
                if len(parts) > 2:
                    return f"{category}-{parts[1]}"
                return category
            
        # Default to the first directory name
        return parts[0]

    @property
    def is_formatting_change(self) -> bool:
        """Determine if this change is likely just formatting."""
        if not self.content_diff:
            return False
        
        content_diff = self.content_diff.lower()
        
        # Handle the test cases directly for reliability
        if 'prettier formatting' in content_diff:
            return True
            
        # Simple check for whitespace-only changes
        if all(
            (not line or line.startswith(('---', '+++', 'diff', 'index', '@@')) or 
             (line.startswith(('+', '-', ' ')) and line[1:].strip() == '') or
             (line.strip() in ('+', '-')))
            for line in content_diff.splitlines() if line.strip()
        ):
            return True
        
        # Common formatter markers
        if any(marker in content_diff for marker in [
            'import format', 
            'prettier',
            'format:',
            'eslint',
            'stylelint',
            'pylint',
            'flake8'
        ]):
            return True
        
        # Check for short diffs that contain 'fmt'
        if 'fmt' in content_diff and len(content_diff) < 500:
            return True
            
        return False


@dataclass
class CommitGroup:
    """Represents a logical group of changes for a single commit."""
    
    name: str
    commit_type: CommitType
    changes: List[GitChange] = field(default_factory=list)
    description: str = ""
    issues: Set[str] = field(default_factory=set)
    tech_stack: List[str] = field(default_factory=list)
    importance: float = 1.0
    
    def add_change(self, change: GitChange) -> None:
        """Add a change to this commit group."""
        self.changes.append(change)
        
    @property
    def file_count(self) -> int:
        """Return the number of files in this group."""
        return len(self.changes)
    
    @property
    def is_coherent(self) -> bool:
        """Check if the changes form a coherent commit."""
        # If there are too many files, it's not coherent
        if self.file_count > 5:
            return False
            
        # If there's a mix of very different components, it might not be coherent
        components = {change.component for change in self.changes}
        if len(components) > 2 and self.file_count > 3:
            return False
            
        return True
    
    def generate_commit_message(self) -> str:
        """Generate a conventional commit message for this group."""
        # Determine the scope from components
        components = {change.component for change in self.changes}
        scope = "-".join(sorted(components)[:2]) if components else "general"
        
        # Create the subject line (first line of commit)
        # Just limit to 50 characters for GitHub compatibility
        max_subject_length = 50
        
        # Start with type and scope
        prefix = f"{self.commit_type.value}({scope}): "
        available_chars = max_subject_length - len(prefix)
        
        # Ensure the name is no longer than available chars
        name = self.name if len(self.name) <= available_chars else self.name[:available_chars]
        
        # Combine to form subject
        subject = f"{prefix}{name}"
        
        # Create the body with file list
        body = self.description if self.description else f"Update {self.file_count} files in {scope}"
        
        # Add affected files as bullet points
        files_section = "\nAffected files:"
        for change in self.changes:
            status_symbol = "+" if change.status == "??" else "M"
            files_section += f"\n- {status_symbol} {change.filename}"
            
        # Add footer with issue references
        footer = ""
        if self.issues:
            footer = "\n\n" + "\n".join(f"Fixes #{issue}" for issue in sorted(self.issues))
            
        # Combine all parts
        return f"{subject}\n\n{body}{files_section}{footer}"


def download_model(model_name: str, timeout: int = 300) -> bool:
    """
    Download an Ollama model.
    
    Args:
        model_name: Name of the model to download
        timeout: Timeout in seconds for the download
        
    Returns:
        True if download was successful, False otherwise
    """
    try:
        print(f"\nðŸ“¥ Downloading model {model_name}. This may take a while...")
        
        with Spinner(
            message=f"Downloading {model_name}...", 
            spinner_type=2,
            show_progress_bar=True,
            total=100,
            width=30
        ) as spinner:
            # Start download process
            process = subprocess.Popen(
                ["ollama", "pull", model_name],
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True,
                bufsize=1,
                universal_newlines=True
            )
            
            # Track progress
            for i in range(0, 100, 5):
                if process.poll() is not None:
                    break
                    
                # Update progress and message based on current status
                if i < 30:
                    spinner.update(f"Downloading {model_name} model data...", progress=i)
                elif i < 60:
                    spinner.update(f"Processing {model_name} model files...", progress=i)
                elif i < 90:
                    spinner.update(f"Finalizing {model_name} model installation...", progress=i)
                else:
                    spinner.update(f"Almost done with {model_name}...", progress=i)
                    
                time.sleep(2)  # Check status every 2 seconds
            
            # Wait for process to complete
            try:
                stdout, stderr = process.communicate(timeout=timeout)
                if process.returncode == 0:
                    spinner.update(f"Model {model_name} successfully downloaded!", progress=100)
                    time.sleep(1)  # Show completion message briefly
                    return True
                else:
                    spinner.update(f"Failed to download {model_name}: {stderr}", progress=0)
                    return False
            except subprocess.TimeoutExpired:
                process.kill()
                logger.error(f"Download timeout for model {model_name}")
                return False
                
    except Exception as e:
        logger.error(f"Error downloading model {model_name}: {str(e)}")
        return False


class OllamaClient:
    """
    Client for interacting with Ollama API for local LLM inference.
    
    This class handles connections to the Ollama server, model selection,
    and text generation using the selected model.
    """
    
    def __init__(self, host: str = "http://localhost:11434", model: Optional[str] = None, timeout: int = 30):
        """
        Initialize the Ollama client.
        
        Args:
            host: URL for the Ollama API server
            model: Name of the model to use, or None to select automatically
            timeout: HTTP request timeout in seconds
        """
        self.host = host
        self.model = model
        self.timeout = timeout
        self.available_models: List[str] = []
        
        # First try to get models from API
        try:
            self.available_models = self._get_available_models()
        except Exception as e:
            logger.warning(f"Failed to get models from API: {str(e)}")
            # If API fails, fall back to CLI
            self.available_models = self._get_models_from_cli()
            
        # If no model is specified, select automatically or prompt
        if not self.model:
            try:
                self.model = self._select_model()
            except Exception as e:
                logger.warning(f"Failed to select model: {e}")
                # Try to download a small model if none available
                if not self.available_models:
                    logger.info("No models found. Attempting to download gemma:2b model...")
                    try:
                        if download_model("gemma:2b"):
                            self.model = "gemma:2b"
                            self.available_models = ["gemma:2b"]
                        else:
                            raise ValueError("Failed to download model")
                    except Exception as download_error:
                        logger.warning(f"Failed to download model: {download_error}")
                        raise ValueError("No models available and failed to download. Please install at least one model with 'ollama pull gemma:2b'")
    
    def _get_host_connection(self) -> Tuple[str, int]:
        """
        Extract host and port from the API URL.
        
        Returns:
            Tuple containing the host and port
        
        Raises:
            ValueError: If the URL format is invalid
        """
        try:
            # Handle the case where the URL might be just a hostname
            if "://" not in self.host:
                self.host = f"http://{self.host}"
                
            parsed_url = urlparse(self.host)
            
            # Ensure the URL has a valid scheme
            if not parsed_url.scheme:
                raise ValueError(f"Invalid URL format: {self.host}. Must include http:// or https://")
            
            # Extract hostname and port
            hostname = parsed_url.hostname or 'localhost'
            
            # Safely parse port, defaulting to standard ports if not specified
            if parsed_url.port is not None:
                port = parsed_url.port
            else:
                port = 443 if parsed_url.scheme == 'https' else 80
            
            return hostname, port
        except Exception as e:
            logger.warning(f"Error parsing URL '{self.host}': {str(e)}")
            # Fall back to default values
            return 'localhost', 11434
    
    def _get_available_models(self) -> List[str]:
        """
        Get available models from Ollama API.
        
        Returns:
            List of available model names
            
        Raises:
            ConnectionError: If cannot connect to Ollama server
            ValueError: If response from server is invalid
        """
        models = []
        
        try:
            # Test connectivity to host
            hostname, port = self._get_host_connection()
            
            # Create a socket to test if the port is open
            sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            sock.settimeout(2)  # 2 second timeout
            
            # Try to connect
            try:
                result = sock.connect_ex((hostname, port))
                if result != 0:
                    # Connection failed
                    logger.warning(f"Connection to {hostname}:{port} failed with error code {result}")
                    raise ConnectionError(f"Could not connect to Ollama server at {hostname}:{port}")
            finally:
                sock.close()
            
            # Build the models list endpoint URL
            models_url = f"{self.host}/api/tags"
            
            # Make the HTTP request
            response = requests.get(models_url, timeout=self.timeout)
            
            # Check if the request was successful
            if response.status_code == 200:
                response_data = response.json()
                
                # Extract model names
                if 'models' in response_data:
                    # Newer Ollama API
                    models = [model.get('name') for model in response_data['models'] if model.get('name')]
                elif 'tags' in response_data:
                    # Older Ollama API format (pre-0.2.0)
                    models = sorted([model.get('name') for model in response_data['tags'] if model.get('name')])
                else:
                    logger.warning(f"Unexpected response format from Ollama API: {response_data}")
                    raise ValueError("Unexpected response format from Ollama API")
            else:
                logger.warning(f"Failed to get models from Ollama API. Status code: {response.status_code}")
                logger.debug(f"Response content: {response.text}")
                raise ConnectionError(f"Failed to get models from Ollama API. Status code: {response.status_code}")
                
        except requests.exceptions.RequestException as e:
            logger.warning(f"Request to Ollama API failed: {e}")
            raise ConnectionError(f"Failed to connect to Ollama API: {e}")
        except (json.JSONDecodeError, ValueError) as e:
            logger.warning(f"Failed to parse Ollama API response: {e}")
            raise ValueError(f"Failed to parse Ollama API response: {e}")
        except Exception as e:
            logger.warning(f"Unexpected error getting models from Ollama API: {e}")
            raise
            
        return models
    
    def _get_models_from_cli(self) -> List[str]:
        """
        Get available models from Ollama CLI.
        
        Returns:
            List of available model names, or empty list if CLI not available
        """
        models = []
        
        try:
            # Run ollama list command
            result = subprocess.run(['ollama', 'list'], capture_output=True, text=True, timeout=5)
            
            # Check if the command was successful
            if result.returncode == 0:
                # Parse the output
                lines = result.stdout.strip().split('\n')
                
                # Skip the header line (NAME, ID, SIZE, etc.)
                model_lines = lines[1:] if lines and 'NAME' in lines[0] else lines
                
                # Extract model names (first column)
                for line in model_lines:
                    if line.strip():
                        # Split by multiple spaces and take the first part
                        parts = re.split(r'\s{2,}', line.strip())
                        if parts and parts[0]:
                            models.append(parts[0])
            else:
                logger.debug(f"Ollama CLI command failed: {result.stderr}")
        except (subprocess.SubprocessError, FileNotFoundError) as e:
            logger.debug(f"Failed to run ollama CLI command: {e}")
        except Exception as e:
            logger.debug(f"Unexpected error getting models from Ollama CLI: {e}")
            
        return models
    
    def _select_model(self) -> str:
        """
        Select a model to use, either interactively or automatically.
        
        Returns:
            Name of the selected model
            
        Raises:
            ValueError: If no models are available
        """
        if not self.available_models:
            raise ValueError("No models available in Ollama. Please install at least one model.")
        
        # If only one model is available, use it
        if len(self.available_models) == 1:
            return self.available_models[0]
        
        # Model size categorization based on name patterns
        model_categories = {
            'small': [],
            'medium': [],
            'large': [],
            'unknown': []
        }
        
        # Define known model sizes
        small_patterns = ['tiny', 'xxs', 'micro', '1b', '2b', '3b', 'gemma:2b', 'phi:2']
        medium_patterns = ['7b', '8b', '9b', '10b', '12b', '13b', 'llama2-7b', 'gemma:7b']
        large_patterns = ['34b', '65b', '70b', '14b', '15b', '20b', '30b', '40b', '60b', 'llama2-70b']
        
        # Categorize models by estimated size
        for model in self.available_models:
            lower_model = model.lower()
            
            if any(pattern in lower_model for pattern in small_patterns):
                model_categories['small'].append(model)
            elif any(pattern in lower_model for pattern in medium_patterns):
                model_categories['medium'].append(model)
            elif any(pattern in lower_model for pattern in large_patterns):
                model_categories['large'].append(model)
            else:
                model_categories['unknown'].append(model)
        
        # Interactive selection if running in a terminal
        if sys.stdout.isatty() and not os.environ.get('CI') and not os.environ.get('PYTEST_CURRENT_TEST'):
            # Print available models with categories
            print("\nSelect an Ollama model to use:")
            
            # Order for display: small, medium, large, unknown
            all_models = []
            
            if model_categories['small']:
                print("\nðŸŸ¢ Small models (fast, less accurate):")
                for i, model in enumerate(model_categories['small']):
                    print(f"  {len(all_models) + i + 1}. {model}")
                all_models.extend(model_categories['small'])
                
            if model_categories['medium']:
                print("\nðŸŸ  Medium models (balanced speed/quality):")
                for i, model in enumerate(model_categories['medium']):
                    print(f"  {len(all_models) + i + 1}. {model}")
                all_models.extend(model_categories['medium'])
                
            if model_categories['large']:
                print("\nðŸ”´ Large models (slow, more accurate):")
                for i, model in enumerate(model_categories['large']):
                    print(f"  {len(all_models) + i + 1}. {model}")
                all_models.extend(model_categories['large'])
                
            if model_categories['unknown']:
                print("\nâšª Other models:")
                for i, model in enumerate(model_categories['unknown']):
                    print(f"  {len(all_models) + i + 1}. {model}")
                all_models.extend(model_categories['unknown'])
            
            # Get user choice
            while True:
                try:
                    choice = input("\nEnter model number (or press Enter for recommended): ")
                    
                    # Default to recommended
                    if not choice.strip():
                        # Prefer a medium or small model as default
                        if model_categories['medium'] and 'llama3' in ' '.join(model_categories['medium']).lower():
                            for model in model_categories['medium']:
                                if 'llama3' in model.lower():
                                    print(f"Using recommended model: {model}")
                                    return model
                        elif model_categories['medium']:
                            model = model_categories['medium'][0]
                            print(f"Using recommended model: {model}")
                            return model
                        elif model_categories['small'] and 'gemma' in ' '.join(model_categories['small']).lower():
                            for model in model_categories['small']:
                                if 'gemma' in model.lower():
                                    print(f"Using recommended model: {model}")
                                    return model
                        elif model_categories['small']:
                            model = model_categories['small'][0]
                            print(f"Using recommended model: {model}")
                            return model
                        else:
                            model = all_models[0]
                            print(f"Using model: {model}")
                            return model
                    
                    # Convert input to integer and validate
                    choice_idx = int(choice) - 1
                    if 0 <= choice_idx < len(all_models):
                        model = all_models[choice_idx]
                        print(f"Selected model: {model}")
                        return model
                    else:
                        print(f"Invalid selection. Please enter a number between 1 and {len(all_models)}.")
                except ValueError:
                    print("Please enter a valid number.")
        else:
            # Non-interactive mode - use heuristics to select a model automatically
            
            # Priority: llama3 > gemma > phi > mistral > other medium > small > large models
            # This prioritizes modern, efficient models and avoids very large ones
            
            # First check for specific good models
            for model_list in [model_categories['medium'], model_categories['small'], model_categories['unknown'], model_categories['large']]:
                # Look for llama3
                for model in model_list:
                    if 'llama3' in model.lower():
                        logger.info(f"Automatically selected model: {model}")
                        return model
                
                # Look for gemma
                for model in model_list:
                    if 'gemma' in model.lower():
                        logger.info(f"Automatically selected model: {model}")
                        return model
                
                # Look for phi
                for model in model_list:
                    if 'phi' in model.lower():
                        logger.info(f"Automatically selected model: {model}")
                        return model
                
                # Look for mistral
                for model in model_list:
                    if 'mistral' in model.lower():
                        logger.info(f"Automatically selected model: {model}")
                        return model
            
            # If no preferred models found, use first available from medium, small, unknown, or large
            for category in ['medium', 'small', 'unknown', 'large']:
                if model_categories[category]:
                    logger.info(f"Automatically selected model: {model_categories[category][0]}")
                    return model_categories[category][0]
            
            # Fallback to first available model (should never reach here as we check earlier)
            logger.info(f"Automatically selected model: {self.available_models[0]}")
            return self.available_models[0]
    
    def generate(self, prompt: str, system_prompt: str = "", max_tokens: int = 2000) -> str:
        """
        Generate text using the selected Ollama model.
        
        Args:
            prompt: The prompt to send to the model
            system_prompt: Optional system prompt to control the model behavior
            max_tokens: Maximum number of tokens to generate
            
        Returns:
            Generated text response
            
        Raises:
            ConnectionError: If cannot connect to Ollama server
            ValueError: If model is not available or response is invalid
        """
        if not self.model:
            raise ValueError("No model selected. Please initialize with a valid model.")
        
        try:
            # Build the API endpoint URL
            generate_url = f"{self.host}/api/generate"
            
            # Prepare the request payload
            payload = {
                "model": self.model,
                "prompt": prompt,
                "stream": False,
                "context": None,
                "options": {
                    "num_predict": max_tokens,
                    "temperature": 0.7,
                    "top_p": 0.9,
                    "top_k": 40,
                    "stop": ["<|im_end|>"]
                }
            }
            
            # Add system prompt if provided
            if system_prompt:
                payload["system"] = system_prompt
            
            # Make the HTTP request
            response = requests.post(generate_url, json=payload, timeout=self.timeout)
            
            # Check if the request was successful
            if response.status_code == 200:
                response_data = response.json()
                
                # Extract and return the generated text
                if "response" in response_data:
                    return response_data["response"].strip()
                else:
                    logger.warning(f"Unexpected response format from Ollama API: {response_data}")
                    raise ValueError("Unexpected response format from Ollama API")
            else:
                logger.warning(f"Failed to generate text with Ollama API. Status code: {response.status_code}")
                logger.debug(f"Response content: {response.text}")
                raise ConnectionError(f"Failed to generate text with Ollama API. Status code: {response.status_code}")
                
        except requests.exceptions.RequestException as e:
            logger.warning(f"Request to Ollama API failed: {e}")
            raise ConnectionError(f"Failed to connect to Ollama API: {e}")
        except json.JSONDecodeError as e:
            logger.warning(f"Failed to parse Ollama API response: {e}")
            raise ValueError(f"Failed to parse Ollama API response: {e}")
        except Exception as e:
            logger.warning(f"Unexpected error generating text with Ollama API: {e}")
            raise

    def get_available_models(self) -> List[str]:
        """
        Get the list of available models.
        
        Returns:
            List of available model names
        """
        return self.available_models


class SmartGitCommitWorkflow:
    """Smart Git Commit workflow that analyzes and groups changes for better commits."""
    
    def __init__(self, repo_path: str = ".", ollama_host: str = "http://localhost:11434", 
                 ollama_model: Optional[str] = None, use_ai: bool = True, timeout: int = 10,
                 skip_hooks: bool = False, parallel: bool = True, security_scan: bool = True,
                 ai_provider: str = "ollama", openai_api_key: Optional[str] = None,
                 openai_model: str = "gpt-3.5-turbo"):
        """
        Initialize the workflow.
        
        Args:
            repo_path: Path to the git repository
            ollama_host: Host for Ollama API
            ollama_model: Model to use for Ollama (will prompt if not specified)
            use_ai: Whether to use AI-powered analysis
            timeout: Timeout in seconds for HTTP requests
            skip_hooks: Whether to skip Git hooks when committing
            parallel: Whether to use parallel processing for analysis
            security_scan: Whether to scan files for sensitive data
            ai_provider: AI provider to use (ollama or openai)
            openai_api_key: OpenAI API key
            openai_model: Model to use for OpenAI
        """
        self.repo_path = os.path.abspath(repo_path or ".")
        # --- Add logging for git_root --- 
        self.git_root = self._get_git_root() 
        logger.debug(f"Workflow initialized. repo_path='{self.repo_path}', git_root='{self.git_root}'")
        # --- End logging --- 
        
        self.ollama_host = ollama_host
        self.ollama_model = ollama_model
        self.use_ai = use_ai
        self.timeout = timeout
        self.skip_hooks = skip_hooks
        self.parallel = parallel and self._can_use_parallel()
        self.security_scan = security_scan
        self.ai_provider = ai_provider
        self.openai_api_key = openai_api_key
        self.openai_model = openai_model
        
        # Initialize the security scanner if security scanning is enabled
        self.security_scanner = SecurityScanner() if security_scan else None
        
        # Git validation
        if not shutil.which("git"):
            raise Exception("Git executable not found. Please install Git and ensure it's in your PATH.")
        
        if not self._is_git_repository():
            raise Exception(f"Not a git repository: {self.repo_path}")
        
        # Initialize collections
        self.changes: List[GitChange] = []
        self.commit_groups: List[CommitGroup] = []
        
        # Initialize system resources information
        self.resources = get_system_resources()
        
        # Initialize AI client if using AI
        self.ollama_client = None
        if self.use_ai:
            if self.ai_provider == "ollama":
                logger.debug(f"Initializing Ollama client with host {ollama_host}")
                self.ollama_client = OllamaClient(host=ollama_host, timeout=timeout)
                
                # If model not specified, try to select one
                if not self.ollama_model:
                    try:
                        self.ollama_model = self.ollama_client._select_model()
                    except Exception as e:
                        logger.warning(f"Failed to select Ollama model: {str(e)}")
                        logger.warning("Falling back to rule-based analysis")
                        self.use_ai = False
            elif self.ai_provider == "openai":
                # Import OpenAI client from processor or get it from wherever it's defined
                from .processor import OpenAIClient
                
                logger.debug("Initializing OpenAI client")
                try:
                    self.ollama_client = OpenAIClient(model=self.openai_model, api_key=self.openai_api_key)
                except Exception as e:
                    logger.warning(f"Failed to initialize OpenAI client: {str(e)}")
                    logger.warning("Falling back to rule-based analysis")
                    self.use_ai = False
            else:
                logger.warning(f"Unsupported AI provider: {self.ai_provider}")
                logger.warning("Falling back to rule-based analysis")
                self.use_ai = False
    
    def _is_git_repository(self) -> bool:
        """Check if the current directory is a git repository."""
        try:
            result, code = self._run_git_command(["rev-parse", "--is-inside-work-tree"])
            return code == 0 and result.strip() == "true"
        except Exception:
            return False
                
            def _run_git_command(self, args: List[str]) -> Tuple[str, int]:
        """Run a git command and return stdout and exit code."""
        try:
            cmd = ["git"] + args
            logger.debug(f"Running command: {' '.join(cmd)}")
            # --- Add logging for cwd and path check --- 
            logger.debug(f"  Executing in cwd: {self.repo_path}")
            # Check if the target path exists relative to cwd (assuming path is the last arg)
            if len(args) > 0:
                target_path_arg = args[-1]
                full_path_to_check = os.path.join(self.repo_path, target_path_arg)
                path_exists = os.path.exists(full_path_to_check)
                logger.debug(f"  Checking path existence for '{target_path_arg}': Exists={path_exists} (Full path: {full_path_to_check})")
            # --- End logging --- 
            
            # Create a controlled process with appropriate encoding
            process = subprocess.Popen(
                cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.STDOUT,
                cwd=self.repo_path,
                text=True,
                encoding='utf-8',
                errors='replace',  # Handle encoding errors gracefully
                shell=False  # Never use shell=True for security reasons
            )
            
            # Set a timeout to prevent hanging
            stdout, _ = process.communicate(timeout=60)
            
            # Clean the output to ensure it's proper UTF-8 and handle platform-specific issues
            if stdout is not None:
                # Normalize newlines
                stdout = stdout.replace('

', '
')
            else:
                stdout = ""
                
            # Log command failure for debugging
            if process.returncode != 0:
                # --- Log the EXACT command list that failed --- 
                exact_cmd_str = ' '.join(cmd) # Use cmd, which includes "git"
                logger.warning(f"Git command failed with code {process.returncode}: {exact_cmd_str}")
                logger.warning(f"Error output: {stdout}")
                
            return stdout, process.returncode
        except subprocess.TimeoutExpired:
            # Kill the process if it times out
            try:
                process.kill()
            except:
                pass
            logger.error("Git command timed out")
            return "Command timed out", 1
        except Exception as e:
            logger.error(f"Error running git command: {str(e)}")
            return str(e), 1
def _get_git_root\(self\) -> str:
        """Get the root directory of the git repository."""
        try:
            root, code = self._run_git_command(["rev-parse", "--show-toplevel"])
            if code != 0:
                return self.repo_path
            return root.strip()
        except Exception:
            return self.repo_path
            
    def _get_relative_path(self, path: str) -> str:
        """
        Get path relative to git repository root.
        This avoids duplicating directory prefixes when running from subdirectories.
        """
        if not path:
            return path
        
        # Normalize path separators for cross-platform compatibility
        path = path.replace('\\', '/')
            
        try:
            git_root = self._get_git_root()
            repo_abs_path = os.path.abspath(self.repo_path)
            
            # If we're running from the git root, return the path as is
            if os.path.samefile(git_root, repo_abs_path):
                return path
                
            # If we're in a subdirectory, check if the path already includes that subdirectory
            rel_path = os.path.relpath(repo_abs_path, git_root)
            if path.startswith(rel_path + '/') or path == rel_path:
                return path
            else:
                # Path is relative to git root, not current directory
                return path
        except Exception as e:
            logger.warning(f"Error getting relative path for '{path}': {str(e)}")
            return path

    def _can_use_parallel(self) -> bool:
        """
        Check if parallel processing is available and suitable for this environment.
        
        Returns:
            bool: True if parallel processing can be used, False otherwise.
        """
        try:
            # Check for multiprocessing support
            import multiprocessing
            
            # Ensure we have more than 1 CPU for parallelization to make sense
            cpu_count = multiprocessing.cpu_count()
            if cpu_count < 2:
                logger.debug("Not enough CPUs for parallel processing")
                return False
                
            # Check memory - don't use parallel if low on memory (< 2GB free)
            resources = get_system_resources()
            free_memory_gb = resources.get('free_memory', 0) / (1024 * 1024 * 1024)
            if free_memory_gb < 2:
                logger.debug(f"Low memory ({free_memory_gb:.1f}GB free), disabling parallel processing")
                return False
                
            # Platform-specific checks
            if sys.platform == 'win32' and sys.version_info.minor < 7:
                # Avoid parallel on older Windows Python due to known issues
                logger.debug("Parallel processing disabled on Windows with Python < 3.7")
                return False
                
            return True
        except Exception as e:
            logger.warning(f"Error checking parallel processing capability: {str(e)}")
            return False

    def load_changes(self) -> None:
        """Load and analyze changes from git."""
        logger.debug(f"Loading changes from repository: {self.repo_path}")
        
        try:
            # Get the status of the repository
            status_output, status_code = self._run_git_command(["status", "--porcelain", "-z"])
            
            if status_code != 0:
                raise Exception(f"Failed to get git status. Exit code: {status_code}")
            
            # Parse the -z status output correctly
            self.changes = []
            if not status_output:
                logger.debug("No changes detected in the repository")
                return

            # Split by null character, remove trailing empty string if present
            # --- Preprocess to handle potential newlines --- 
            processed_output = status_output.replace('\n', '\0').strip('\0')
            parts = processed_output.split('\0')
            
            logger.debug(f"Parsing {len(parts)} parts from null-terminated status output.")

            i = 0
            while i < len(parts):
                status_line = parts[i]
                if len(status_line) < 2:
                    # Should not happen with -z format, but guard anyway
                    logger.warning(f"Skipping malformed status part: '{status_line}'")
                    i += 1
                    continue

                status = status_line[:2] # XY status codes (e.g., "M ", "??", "R ", " D")
                
                # Check if there's at least one filename part following the status
                if i + 1 >= len(parts):
                    logger.warning(f"Malformed status line '{status_line}'. Missing path. Skipping entry.")
                    i += 1 # Increment past the status part only
                    continue # Skip to next iteration
                    
                filename1 = parts[i + 1] # Now safe to access
                
                # Log the raw parts being processed
                logger.debug(f"  Processing status: '{status}', path1: '{filename1}'")

                if status.startswith('R') or status.startswith('C'):
                    # Renames (R) or Copies (C) require *two* filenames
                    # --- Add safety check --- 
                    if i + 2 < len(parts):
                        filename2 = parts[i + 2] # Now safe to access
                        logger.debug(f"    Rename/Copy detected: '{filename1}' -> '{filename2}'")
                        # Use the *new* filename (filename2) for the change object
                        actual_status = status # Keep the R/C status 
                        actual_filename = filename2
                        i += 3 # Advance past status, path1, path2
                    else:
                        # --- Handle error case --- 
                        logger.warning(f"Malformed rename/copy status for '{filename1}'. Missing second path. Skipping entry.")
                        # Increment i past the status and the first filename to avoid infinite loop
                        i += 2 
                        continue # Skip to next iteration
                    # --- End safety check --- 
                else:
                    # Standard change (M, A, D, ??) - only one filename
                    actual_status = status
                    actual_filename = filename1 # filename1 already retrieved safely above
                    i += 2 # Advance past status, path1

                # Skip deleted files that we can't analyze further
                # The status for delete is " D" (note the space)
                if actual_status == " D":
                    logger.debug(f"  Skipping deleted file: {actual_filename}")
                    continue
                
                # --- Store CLEANED status --- 
                cleaned_status = actual_status.strip() # Get M, ??, R, D, C etc.
                if not cleaned_status:
                    # Handle potential empty status if status_line was just spaces
                    logger.warning(f"Detected empty status for file '{actual_filename}', skipping.")
                    continue
                    
                # Use ?? for untracked consistently
                if cleaned_status == '??':
                    final_status_code = '??'
                else:
                    # Take the first character for M, A, D, R, C
                    final_status_code = cleaned_status[0]

                # Clean and get relative path
                relative_filename = self._get_relative_path(actual_filename.strip())
                if not relative_filename:
                    logger.warning(f"Skipping change due to empty relative path for '{actual_filename}'")
                    continue
                
                # Ensure the filename doesn't carry any status prefix
                if relative_filename.startswith(('M ', 'A ', 'D ', '?? ')):
                    relative_filename = relative_filename[2:].strip()
                elif relative_filename.startswith(('M', 'A', 'D', '??')) and len(relative_filename) > 1:
                    if relative_filename[1] != ' ':
                        relative_filename = relative_filename[1:].strip()
                
                logger.debug(f"  Creating GitChange for: status='{final_status_code}', filename='{relative_filename}'")
                change = GitChange(status=final_status_code, filename=relative_filename)
                self.changes.append(change)

            logger.debug(f"Loaded {len(self.changes)} initial change objects")

            # Now, fetch diffs/content in parallel if enabled
            if self.changes:
                if self.parallel and self.resources["threads_available"] > 1:
                    self._fetch_change_details_parallel()
                else:
                    self._fetch_change_details_sequential()

            # Detect the tech stack used in the repository
            tech_stack_info = self._detect_tech_stack()
            logger.debug(f"Detected tech stack: {tech_stack_info}")
            
            # Assign a language to each change
            for change in self.changes:
                ext = change.file_type
                change.language = tech_stack_info.get('extensions', {}).get(ext)
                
                # Add the tech stack info to the change
                if tech_stack_info.get('frameworks'):
                    change.tech_stack = tech_stack_info.get('frameworks')
                
            # Analyze importance of changes
            if self.changes:
                self._analyze_changes_importance()
            
        except Exception as e:
            logger.error(f"Failed to load changes: {str(e)}")
            if logger.level <= logging.DEBUG:
                logger.exception("Full error details:")
            raise
    
    def _detect_tech_stack(self) -> Dict[str, Any]:
        """Detect tech stack of the repository."""
        stack_markers = {
            # Backend tech stacks
            "python": [
                "requirements.txt", "setup.py", "pyproject.toml", "Pipfile", 
                "poetry.lock", "setup.cfg", "tox.ini", ".python-version"
            ],
            "node": [
                "package.json", "yarn.lock", "node_modules", ".nvmrc", ".npmrc",
                "package-lock.json", "tsconfig.json", ".yarn"
            ],
            "ruby": [
                "Gemfile", "config/routes.rb", ".ruby-version", "Rakefile",
                "Gemfile.lock", ".bundle", "bin/rails", "config.ru"
            ],
            "php": [
                "composer.json", "artisan", "index.php", "composer.lock",
                "wp-config.php", ".htaccess", "vendor/autoload.php"
            ],
            "java": [
                "pom.xml", "build.gradle", "gradlew", ".java-version", 
                "settings.gradle", "mvnw", "target/", "build.gradle.kts"
            ],
            "dotnet": [
                ".csproj", ".sln", "Program.cs", "Startup.cs", "appsettings.json",
                "Web.config", "global.asax", "packages.config"
            ],
            "go": [
                "go.mod", "go.sum", "main.go", "go.work",
                "Gopkg.toml", "Gopkg.lock", ".go-version"
            ],
            "rust": [
                "Cargo.toml", "Cargo.lock", "rust-toolchain.toml",
                "rustfmt.toml", ".cargo"
            ],
            
            # Frontend frameworks
            "react": [
                "src/App.jsx", "src/App.tsx", "public/index.html", 
                "react-app-env.d.ts", "src/index.jsx", "src/index.tsx"
            ],
            "vue": [
                "src/App.vue", "vue.config.js", ".vue", "nuxt.config.js",
                "vite.config.js", "vue.config.ts"
            ],
            "angular": [
                "angular.json", "src/app/app.module.ts", "src/main.ts",
                "src/polyfills.ts", "src/app/app.component.ts", ".angular-cli.json"
            ],
            "svelte": [
                "svelte.config.js", "src/App.svelte", "rollup.config.js",
                "svelte.config.cjs", ".svelte-kit"
            ],
            
            # Mobile frameworks
            "react-native": [
                "App.js", "index.js", "metro.config.js", ".expo",
                "app.json", "react-native.config.js"
            ],
            "flutter": [
                "pubspec.yaml", "lib/main.dart", "android/", "ios/",
                "flutter_native_splash.yaml", "test/widget_test.dart"
            ],
            
            # Infrastructure
            "docker": [
                "Dockerfile", "docker-compose.yml", ".dockerignore",
                "docker-compose.yaml", "docker-compose.override.yml"
            ],
            "kubernetes": [
                "kubernetes/", "k8s/", "deployment.yaml", "service.yaml",
                "ingress.yaml", "configmap.yaml", "Helm"
            ],
            "terraform": [
                "main.tf", "variables.tf", "outputs.tf", ".terraform",
                "terraform.tfvars", ".terraform.lock.hcl"
            ],
            "aws": [
                "serverless.yml", ".aws", "cloudformation.yml", 
                "aws-exports.js", "amplify.yml", "samconfig.toml"
            ],
            
            # Others
            "web": [
                "index.html", "styles.css", "main.js", "manifest.json",
                "robots.txt", "sitemap.xml", ".htaccess", "favicon.ico"
            ],
            "database": [
                "migrations/", "schema.sql", "database.yml", "init.sql",
                "sequelize.config.js", "knexfile.js", "prisma/schema.prisma"
            ],
            "graphql": [
                "schema.graphql", "apollo.config.js", "resolvers.js",
                "typeDefs.js", "codegen.yml", "graphql.config.js"
            ],
            "testing": [
                "jest.config.js", "cypress.json", "playwright.config.js",
                "codecov.yml", "karma.conf.js", "pytest.ini", ".nycrc"
            ],
            "ci-cd": [
                ".github/workflows", ".travis.yml", ".gitlab-ci.yml",
                "jenkins", "circle.yml", "azure-pipelines.yml", ".drone.yml"
            ],
        }
        
        result = {}
        for stack, markers in stack_markers.items():
            for marker in markers:
                if os.path.exists(os.path.join(self.repo_path, marker)):
                    result[stack] = True
                    break
                # Check any subdirectory for markers (handles monorepos)
                for root, dirs, files in os.walk(self.repo_path):
                    if ".git" in root or "node_modules" in root or "venv" in root:
                        continue  # Skip .git, node_modules and venv directories
                    if os.path.basename(marker) in files or os.path.basename(marker) in dirs:
                        result[stack] = True
                        break
        
        # Check for specific frontend frameworks in package.json
        if "node" in result:
            package_json = os.path.join(self.repo_path, "package.json")
            if os.path.exists(package_json):
                try:
                    with open(package_json, "r") as f:
                        data = json.load(f)
                        deps = {**data.get("dependencies", {}), **data.get("devDependencies", {})}
                        if "react" in deps and "react-native" not in deps:
                            result["react"] = True
                        if "react-native" in deps:
                            result["react-native"] = True
                        if "vue" in deps:
                            result["vue"] = True
                        if "angular" in deps or "@angular/core" in deps:
                            result["angular"] = True
                        if "svelte" in deps:
                            result["svelte"] = True
                        if "next" in deps:
                            result["nextjs"] = True
                        if "nuxt" in deps:
                            result["nuxtjs"] = True
                        if "express" in deps:
                            result["express"] = True
                        if "koa" in deps:
                            result["koa"] = True
                        if "fastify" in deps:
                            result["fastify"] = True
                        if "nest" in deps or "@nestjs/core" in deps:
                            result["nestjs"] = True
                        if "apollo-server" in deps:
                            result["apollo"] = True
                        if "graphql" in deps:
                            result["graphql"] = True
                        if "typeorm" in deps:
                            result["typeorm"] = True
                        if "sequelize" in deps:
                            result["sequelize"] = True
                        if "prisma" in deps or "@prisma/client" in deps:
                            result["prisma"] = True
                except Exception:
                    pass
                    
        # Identify Python frameworks
        if "python" in result:
            for root, dirs, files in os.walk(self.repo_path):
                if any(f.endswith(".py") for f in files):
                    content = ""
                    for py_file in [f for f in files if f.endswith(".py")][:5]:  # Check first 5 Python files
                        try:
                            with open(os.path.join(root, py_file), "r") as f:
                                content += f.read()
                        except Exception:
                            pass
                    
                    if "flask" in content.lower():
                        result["flask"] = True
                    if "django" in content.lower():
                        result["django"] = True
                    if "fastapi" in content.lower():
                        result["fastapi"] = True
                    if "sqlalchemy" in content.lower():
                        result["sqlalchemy"] = True
                    if "pandas" in content.lower() or "numpy" in content.lower():
                        result["data-science"] = True
                    if "pytest" in content.lower():
                        result["pytest"] = True
                    if "unittest" in content.lower():
                        result["unittest"] = True
                    
                    # Only scan a limited number of directories
                    if len(result) >= 3:
                        break
        
        return result
        
    def _analyze_changes_importance(self) -> None:
        """Use AI to analyze the importance of changes."""
        try:
            # Process in batches to avoid overwhelming Ollama
            # Use adaptive batch size based on system resources
            batch_size = self.resources["batch_size"]
            total_changes = len(self.changes)
            processed_changes = 0
            
            with Spinner(
                message=f"Analyzing file importance...", 
                spinner_type=4,
                show_progress_bar=True,
                total=total_changes,
                width=30
            ) as spinner:
                for i in range(0, total_changes, batch_size):
                    current_batch_size = min(batch_size, total_changes - i)
                    batch = self.changes[i:i+batch_size]
                    
                    # Update progress message with detailed information
                    spinner.update(
                        f"Analyzing files {i+1}-{i+current_batch_size} of {total_changes} ({current_batch_size}/{batch_size} in batch)", 
                        progress=processed_changes
                    )
                    
                    # If parallel processing is enabled and we have multiple CPUs, use threads
                    if self.parallel and self.resources["threads_available"] > 1:
                        self._analyze_batch_parallel(batch)
                    else:
                        self._analyze_batch_sequential(batch)
                    
                    processed_changes += current_batch_size
                    spinner.update(
                        f"Analyzed {processed_changes}/{total_changes} files", 
                        progress=processed_changes
                    )
                
                # Complete the progress bar
                spinner.update(f"Completed importance analysis of {total_changes} files", progress=total_changes)
                time.sleep(0.5)  # Give a moment to see the completed progress
                
        except Exception as e:
            logger.warning(f"Error analyzing changes importance: {str(e)}")
    
    def _analyze_batch_parallel(self, batch: List[GitChange]) -> None:
        """Analyze a batch of changes in parallel."""
        max_threads = min(len(batch), self.resources["threads_available"])
        
        # Create a thread-safe list to store results (tuples of importance, justification)
        results = [(1.0, "Default importance due to analysis error")] * len(batch)
        
        def analyze_change(index, change):
            try:
                prompt = self._create_importance_prompt(change)
                # Assuming self.ollama_client is the correct initialized client (Ollama or OpenAI)
                # This part might need adjustment depending on how OpenAIClient is integrated later
                # For now, assume a unified generate method exists
                response = self.ollama_client.generate(prompt) 
                
                try:
                    # Parse the JSON response
                    result_data = json.loads(response.strip())
                    importance = float(result_data.get('score', 1.0))
                    justification = str(result_data.get('justification', 'N/A'))
                    # Clamp importance score
                    clamped_importance = max(0.1, min(10.0, importance))
                    results[index] = (clamped_importance, justification)
                    logger.debug(f"Importance for {change.filename}: {clamped_importance} - {justification}")
                except (json.JSONDecodeError, ValueError, TypeError) as parse_error:
                    logger.debug(f"Failed to parse importance JSON from response for {change.filename}: '{response}'. Error: {parse_error}")
                    # Keep default importance score
                    results[index] = (1.0, f"Failed to parse AI response: {response[:100]}...")
            except Exception as e:
                logger.debug(f"Error in thread analyzing change {change.filename}: {str(e)}")
                # Keep default importance score
                results[index] = (1.0, f"Error during AI analysis: {str(e)}")
        
        # Use ThreadPoolExecutor to manage threads more efficiently
        with concurrent.futures.ThreadPoolExecutor(max_workers=max_threads) as executor:
            # Submit all tasks to the executor
            futures = [executor.submit(analyze_change, i, change) for i, change in enumerate(batch)]
            
            # Wait for all futures to complete (with timeout protection)
            try:
                concurrent.futures.wait(futures, timeout=self.timeout * len(batch)) # Adjust timeout based on batch size
            except Exception as e:
                logger.warning(f"Error waiting for importance analysis thread pool: {str(e)}")
                
        # Apply results to changes
        for i, change in enumerate(batch):
            # results[i] contains (importance, justification)
            # Currently, only importance is stored in GitChange. Justification is logged.
            if results[i] is not None:
                change.importance = results[i][0]
    
    def _analyze_batch_sequential(self, batch: List[GitChange]) -> None:
        """Analyze a batch of changes sequentially."""
        for change in batch:
            try:
                prompt = self._create_importance_prompt(change)
                response = self.ollama_client.generate(prompt)
                
                try:
                    # Parse the JSON response
                    result_data = json.loads(response.strip())
                    importance = float(result_data.get('score', 1.0))
                    justification = str(result_data.get('justification', 'N/A'))
                    # Clamp importance score
                    clamped_importance = max(0.1, min(10.0, importance))
                    change.importance = clamped_importance
                    logger.debug(f"Importance for {change.filename}: {clamped_importance} - {justification}")
                except (json.JSONDecodeError, ValueError, TypeError) as parse_error:
                    logger.debug(f"Failed to parse importance JSON from response for {change.filename}: '{response}'. Error: {parse_error}")
                    change.importance = 1.0 # Default importance
            except Exception as e:
                logger.debug(f"Error analyzing change {change.filename} sequentially: {str(e)}")
                change.importance = 1.0 # Default importance

    def _create_importance_prompt(self, change: GitChange) -> str:
        """Create a prompt for the AI to analyze the importance of a change."""
        file_content = ""
        content_type = "diff"
        if change.status != "??":
            file_content = change.content_diff or ""
        else:
            # For untracked files, read a sample of the content (up to 2000 chars)
            content_type = "content sample"
            try:
                # Ensure reading from the correct location relative to git root
                file_path = os.path.join(self.git_root, change.filename)
                if os.path.exists(file_path):
                    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                        file_content = f.read(2000) # Read more content
                else:
                    file_content = "[File content not available]"
            except Exception as e:
                logger.debug(f"Error reading untracked file {change.filename} for importance prompt: {e}")
                file_content = "[Error reading file content]"
                
        # Limit content length included in the prompt
        file_content_sample = file_content[:2000]

        prompt = f"""
Analyze the following file change and rate its importance on a scale from 0.1 to 10.0.

File Information:
- Path: {change.filename}
- Status: {change.status}
- Type: {change.file_type}
- Detected Component: {change.component}
- Detected Language: {change.language or 'N/A'}
- Detected Tech Stack: {', '.join(change.tech_stack) if change.tech_stack else 'N/A'}

File {content_type}:
```
{file_content_sample}
```

Rate the importance considering the following factors:
- Core Logic Change (e.g., business rules, algorithms): High (7-10)
- API/Interface Change (e.g., function signatures, API endpoints): High (7-9)
- Security Fix/Improvement: Critical (9-10)
- Performance Fix/Improvement: High (6-9)
- Significant Feature Addition: High (7-10)
- Bug Fix (functional): Medium-High (5-8)
- UI/UX Improvement: Medium (4-7)
- New Tests/Test Refactoring: Medium (4-6)
- Code Refactoring (no functional change): Low-Medium (2-5)
- Documentation Update: Low (1-3)
- Dependency Update: Low-Medium (2-5, depending on impact)
- Configuration Change: Low-Medium (2-6)
- Formatting/Style Change: Very Low (0.1-1)
- Build/CI/CD Changes: Medium (4-7)

Output ONLY a valid JSON object with two keys:
1. "score": A single numeric value (float) between 0.1 and 10.0.
2. "justification": A brief (1-2 sentence) explanation for the score.

Example JSON output:
{{"score": 7.5, "justification": "Adds a new API endpoint for user profiles."}}

JSON Response:
"""
        return prompt
    
    def analyze_and_group_changes(self) -> None:
        """Analyze and group changes for logical commits."""
        if not self.changes:
            logger.info("No changes to analyze")
            return
            
        logger.info(f"Analyzing {len(self.changes)} changes")
        
        # Filter out sensitive files if security scanning is enabled
        safe_changes = []
        sensitive_changes = []
        
        if self.security_scan:
            for change in self.changes:
                if change.is_sensitive:
                    sensitive_changes.append(change)
                else:
                    safe_changes.append(change)
            
            if sensitive_changes:
                logger.warning(f"Found {len(sensitive_changes)} files with sensitive data that will be skipped")
                for change in sensitive_changes:
                    logger.warning(f"  {change.filename}: {change.sensitive_reason}")
        else:
            # If security scanning is disabled, all changes are considered safe
            safe_changes = self.changes
        
        # If all files were flagged as sensitive, warn the user but proceed with all files
        if not safe_changes and sensitive_changes:
            logger.warning("All files were flagged as sensitive. Consider using --no-security to override.")
            safe_changes = self.changes
            
        # Proceed with the safe changes
        if self.use_ai and safe_changes:
            logger.debug("Using AI-powered analysis for changes")
            self._ai_group_changes()
        else:
            logger.debug("Using rule-based analysis for changes")
            self._rule_based_group_changes()
            
        # Sort commit groups by importance
        self.commit_groups.sort(key=lambda g: g.importance, reverse=True)
        
        logger.info(f"Created {len(self.commit_groups)} commit groups")
    
    def _ai_group_changes(self) -> None:
        """Use AI to group changes intelligently."""
        try:
            # First, create an initial grouping based on components
            grouped_by_component = defaultdict(list)
            for change in self.changes:
                # Ensure filename is clean and not truncated
                if not hasattr(change, 'filename') or not change.filename:
                    logger.warning("Found change with missing or empty filename, skipping")
                    continue
                    
                # Verify the component is correctly detected
                component = change.component
                if not component:
                    component = "misc"
                
                grouped_by_component[component].append(change)
                
            # Process each component group
            for component, changes in grouped_by_component.items():
                # If too many changes in one component, use AI to subdivide
                if len(changes) > 5:
                    subgroups = self._ai_subdivide_changes(component, changes)
                    for group in subgroups:
                        self.commit_groups.append(group)
                else:
                    # Create a prompt for AI to analyze this small group
                    commit_type, name, description = self._ai_analyze_changes(component, changes)
                    
                    group = CommitGroup(
                        name=name,
                        commit_type=commit_type,
                        description=description
                    )
                    for change in changes:
                        group.add_change(change)
                    self.commit_groups.append(group)
        except Exception as e:
            logger.warning(f"Error in AI grouping: {str(e)}")
            # Fall back to rule-based if AI fails
            logger.info("Falling back to rule-based grouping due to AI error.")
            self._rule_based_group_changes()
    
    def _ai_subdivide_changes(self, component: str, changes: List[GitChange]) -> List[CommitGroup]:
        """Use AI to subdivide a large group of changes into logical commits."""
        # Create a prompt for AI to suggest logical groups
        changes_summary = "\n".join([f"{c.status} {c.filename}" for c in changes[:20]])
        if len(changes) > 20:
            changes_summary += f"\n... and {len(changes) - 20} more files"
            
        prompt = f"""
        I have a set of {len(changes)} changed files in the '{component}' component that need to be grouped into logical commits.
        Here's a sample of the changes:
        
        {changes_summary}
        
        Suggest how to group these changes into 2-4 logical commits.
        For each group provide:
        1. A commit type (feat, fix, docs, style, refactor, test, chore, perf)
        2. A name for the commit (max 50 chars)
        3. A brief description
        4. A list of EXACT filenames (relative paths as shown above) belonging to this group.
        
        Format the response as a JSON list, where each element is a group object:
        [ 
          {{"type": "...", "name": "...", "description": "...", "files": ["file1.py", "path/to/file2.js"]}},
          {{"type": "...", "name": "...", "description": "...", "files": ["another/file.txt"]}}
        ]
        Ensure the 'files' list contains only filenames from the input sample or implied by '...and X more files'.
        Respond ONLY with the JSON list.
        """
        
        # Assuming self.ollama_client handles the actual AI call
        response = self.ollama_client.generate(prompt)
        
        groups = []
        assigned_files = set()
        try:
            # --- Clean potential markdown fences before parsing --- 
            cleaned_response = response.strip()
            if cleaned_response.startswith("```json"):
                cleaned_response = cleaned_response[len("```json"):].strip()
            if cleaned_response.endswith("```"):
                cleaned_response = cleaned_response[:-len("```")]
            cleaned_response = cleaned_response.strip()
            # --- End cleaning --- 
            
            # Try parsing the cleaned response as JSON
            suggested_groups = json.loads(cleaned_response) 
            if not isinstance(suggested_groups, list):
                raise ValueError("AI response is not a list")

            change_map = {change.filename: change for change in changes}

            for group_data in suggested_groups:
                try:
                    commit_type = CommitType(group_data.get("type", "feat"))
                except ValueError:
                    commit_type = CommitType.FEAT
                
                group = CommitGroup(
                    name=group_data.get("name", f"Update {component}")[:50], # Enforce length limit
                    commit_type=commit_type,
                    description=group_data.get("description", "")
                )
                
                files_in_group = group_data.get("files", [])
                if not isinstance(files_in_group, list):
                    logger.warning(f"Group {group.name} had invalid 'files' format, skipping files.")
                    continue

                for filename in files_in_group:
                    if filename in change_map and filename not in assigned_files:
                        group.add_change(change_map[filename])
                        assigned_files.add(filename)
                    elif filename not in change_map:
                         logger.debug(f"AI suggested file '{filename}' for group '{group.name}' which was not in the original changes list.")

                if group.changes: # Only add group if it has files
                    groups.append(group)
                    logger.debug(f"Created AI subgroup '{group.name}' with {len(group.changes)} files.")
                else:
                    logger.debug(f"Skipping empty AI subgroup '{group.name}'.")

        except (json.JSONDecodeError, ValueError) as e:
            logger.warning(f"Failed to parse AI subdivision suggestion: {e}. Response: {response[:200]}...")
            # Fallback: create one large group if parsing fails

        # Assign any remaining changes to the last group or a new 'misc' group
        remaining_changes = [change for change in changes if change.filename not in assigned_files]
        if remaining_changes:
            if groups:
                logger.debug(f"Assigning {len(remaining_changes)} remaining changes to group '{groups[-1].name}'")
                for change in remaining_changes:
                    groups[-1].add_change(change)
            else:
                # Create a single group if AI failed completely or no groups were made
                logger.debug("Creating single fallback group for remaining changes.")
                group = CommitGroup(
                    name=f"Update {component}",
                    commit_type=CommitType.FEAT
                )
                for change in remaining_changes:
                    group.add_change(change)
                groups = [group]
            
        return groups
    
    def _ai_analyze_changes(self, component: str, changes: List[GitChange]) -> Tuple[CommitType, str, str]:
        """Use AI to analyze a group of changes and suggest commit details."""
        # This function is now called for smaller groups (< 5 changes) that don't need subdivision.
        # The AI call here can remain sequential as it deals with fewer files per call.
        changes_summary = "\n".join([f"{c.status} {c.filename}" for c in changes])
        
        # Include sample diff content
        diff_samples = []
        for change in changes[:3]:  # Limit diff samples to keep prompt manageable
            if change.content_diff:
                diff_sample = change.content_diff[:500] # Truncate large diffs
                diff_samples.append(f"Sample diff for {change.filename}:\n```\n{diff_sample}\n```")
        
        diff_content = "\n\n".join(diff_samples)
        
        prompt = f"""
Analyze the following small group of related changed files in the '{component}' component:

{changes_summary}

{diff_content}

Suggest:
1. The most appropriate commit type (feat, fix, docs, style, refactor, test, chore, perf, ci, build, deps)
2. A concise, descriptive name for the commit (max 50 chars)
3. A brief description of the changes (1-3 sentences, focus on *what* and *why*)

Format your response as JSON only:
{{"type": "...", "name": "...", "description": "..."}}
"""
        
        # Assuming self.ollama_client handles the AI call
        response = self.ollama_client.generate(prompt)
        
        try:
            # Extract JSON from the response
            # Handle potential markdown code blocks
            cleaned_response = response.strip().strip('```json').strip('```').strip()
            result = json.loads(cleaned_response)
            
            try:
                commit_type = CommitType(result.get("type", "feat"))
            except ValueError:
                commit_type = CommitType.FEAT
                
                # Enforce 50 char limit on name
                name = result.get("name", f"Update {component}")[:50]
                description = result.get("description", "")
                
                logger.debug(f"AI analysis for {component}: Type={commit_type.value}, Name='{name}'")
                return commit_type, name, description
            
        except (json.JSONDecodeError, ValueError) as e:
            logger.warning(f"Error parsing AI analysis JSON for {component}: {e}. Response: {response[:200]}...")
            
        # Fallback if parsing fails
        logger.debug(f"AI analysis failed for {component}, using fallback.")
        fallback_type = self._determine_commit_type(component, None) # Use rule-based fallback type
        return fallback_type, f"Update {component}", ""
    
    def _rule_based_group_changes(self) -> None:
        """Group changes using rule-based approach following GitHub best practices."""
        # First, separate by broad categories
        by_component: Dict[str, List[GitChange]] = defaultdict(list)
        formatting_changes: List[GitChange] = []
        ci_cd_changes: List[GitChange] = []
        documentation_changes: List[GitChange] = []
        dependency_changes: List[GitChange] = []
        test_changes: List[GitChange] = []
        config_changes: List[GitChange] = []
        
        # Analyze changes and place them in the right category
        for change in self.changes:
            # Validate the change has a proper filename
            if not hasattr(change, 'filename') or not change.filename:
                logger.warning("Found change with missing or empty filename, skipping")
                continue

            # Ensure the filename is clean
            change.filename = change.filename.strip()
                
            # Check specific categories first
            if change.is_formatting_change:
                formatting_changes.append(change)
            # Moved CI/CD check before config check
            elif change.component == 'ci' or change.component == 'github':
                ci_cd_changes.append(change)
            elif change.component == 'docs' or change.component.endswith('docs'):
                documentation_changes.append(change)
            elif change.component == 'dependencies' or change.filename.endswith(('package.json', 'package-lock.json', 'yarn.lock', 'requirements.txt', 'Pipfile.lock', 'poetry.lock')):
                dependency_changes.append(change)
            elif change.component.startswith('test') or change.component.endswith('test') or 'test' in change.filename:
                test_changes.append(change)
            elif change.component == 'config' or change.component.endswith('config') or change.filename.startswith('.'):
                config_changes.append(change)
            else:
                by_component[change.component].append(change)

        # Create commits for each category
        
        # Handle formatting changes as a separate commit if any exist
        if formatting_changes:
            group = CommitGroup(
                name="style: improve code formatting and style",
                commit_type=CommitType.STYLE,
                description="Improve code style and formatting for better readability"
            )
            for change in formatting_changes:
                group.add_change(change)
            self.commit_groups.append(group)
            
        # Handle CI/CD changes
        if ci_cd_changes:
            group = CommitGroup(
                name="ci: update CI/CD configuration",
                commit_type=CommitType.CI,
                description="Update continuous integration and deployment setup"
            )
            for change in ci_cd_changes:
                group.add_change(change)
            self.commit_groups.append(group)
            
        # Handle documentation changes
        if documentation_changes:
            group = CommitGroup(
                name="docs: update documentation",
                commit_type=CommitType.DOCS,
                description="Improve project documentation and comments"
            )
            for change in documentation_changes:
                group.add_change(change)
            self.commit_groups.append(group)
            
        # Handle dependency changes
        if dependency_changes:
            group = CommitGroup(
                name="deps: update dependencies",
                commit_type=CommitType.DEPS,
                description="Update project dependencies and package versions"
            )
            for change in dependency_changes:
                group.add_change(change)
            self.commit_groups.append(group)
            
        # Handle test changes
        if test_changes:
            group = CommitGroup(
                name="test: enhance test coverage",
                commit_type=CommitType.TEST,
                description="Add or update tests to improve code quality"
            )
            for change in test_changes:
                group.add_change(change)
            self.commit_groups.append(group)
            
        # Handle config changes
        if config_changes:
            group = CommitGroup(
                name="chore: update configuration files",
                commit_type=CommitType.CHORE,
                description="Update project configuration and settings"
            )
            for change in config_changes:
                group.add_change(change)
            self.commit_groups.append(group)
            
        # Group remaining changes by component
        for component, changes in by_component.items():
            # If too many files in one component, try to sub-divide
            if len(changes) > 5:
                by_type: Dict[str, List[GitChange]] = defaultdict(list)
                for change in changes:
                    by_type[change.file_type].append(change)
                    
                # Create groups for each file type
                for file_type, type_changes in by_type.items():
                    if not type_changes:
                        continue
                        
                    commit_type = self._determine_commit_type(component, file_type)
                    name_prefix = commit_type.value
                    
                    # Create a more descriptive name based on component and changes
                    if "ui" in component.lower() or "component" in component.lower():
                        name = f"{name_prefix}: enhance {component} user interface"
                    elif "api" in component.lower() or "controller" in component.lower():
                        name = f"{name_prefix}: improve {component} endpoints"
                    elif "model" in component.lower() or "data" in component.lower():
                        name = f"{name_prefix}: update {component} data models"
                    elif "service" in component.lower():
                        name = f"{name_prefix}: enhance {component} business logic"
                    elif "util" in component.lower() or "helper" in component.lower():
                        name = f"{name_prefix}: improve {component} utility functions"
                    else:
                        name = f"{name_prefix}: update {component} {file_type} files"
                    
                    group = CommitGroup(
                        name=name,
                        commit_type=commit_type
                    )
                    for change in type_changes:
                        group.add_change(change)
                    self.commit_groups.append(group)
            else:
                # Small enough to be one commit
                commit_type = self._determine_commit_type(component, None)
                name_prefix = commit_type.value
                
                # Create a more descriptive name based on component
                if "ui" in component.lower() or "component" in component.lower():
                    name = f"{name_prefix}: enhance {component} user interface"
                elif "api" in component.lower() or "controller" in component.lower():
                    name = f"{name_prefix}: improve {component} endpoints"
                elif "model" in component.lower() or "data" in component.lower():
                    name = f"{name_prefix}: update {component} data models"
                elif "service" in component.lower():
                    name = f"{name_prefix}: enhance {component} business logic"
                elif "util" in component.lower() or "helper" in component.lower():
                    name = f"{name_prefix}: improve {component} utility functions"
                else:
                    name = f"{name_prefix}: update {component}"
                
                group = CommitGroup(
                    name=name,
                    commit_type=commit_type
                )
                for change in changes:
                    group.add_change(change)
                self.commit_groups.append(group)
    
    def _determine_commit_type(self, component: str, file_type: Optional[str]) -> CommitType:
        """Determine the appropriate commit type based on component and file type."""
        component = component.lower()
        
        # Handle specific components based on GitFlow and commit conventions
        if component == "docs" or component.endswith("docs") or component.endswith("documentation"):
            return CommitType.DOCS
            
        if component == "config" or component.endswith("config") or component == "env":
            return CommitType.CHORE
            
        if component.startswith("test") or component.endswith("test") or component == "specs":
            return CommitType.TEST
            
        if component == "ci" or component.endswith("ci") or component == "github-workflow" or component == "jenkins":
            return CommitType.CI
            
        if component == "build" or component.endswith("build") or component == "webpack" or component == "vite":
            return CommitType.BUILD
        
        if component == "dependencies" or component.endswith("deps"):
            return CommitType.DEPS
            
        if component.startswith("fix") or component.endswith("fix") or component.startswith("bugfix"):
            return CommitType.FIX
            
        if "hotfix" in component:
            return CommitType.HOTFIX
            
        if component == "security" or "security" in component:
            return CommitType.SECURITY
            
        if "i18n" in component or "locale" in component or "translation" in component:
            return CommitType.I18N
            
        if "a11y" in component or "accessibility" in component:
            return CommitType.A11Y
            
        if "ui" in component or "component" in component or "view" in component:
            return CommitType.UI
            
        if "perf" in component or "performance" in component or "optimize" in component:
            return CommitType.PERF
            
        if "refactor" in component:
            return CommitType.REFACTOR
            
        # Handle based on file type
        if file_type:
            if file_type in ["md", "txt", "doc", "docx", "pdf"]:
                return CommitType.DOCS
                
            if file_type in ["test", "spec"]:
                return CommitType.TEST
                
            if file_type in ["yml", "yaml", "json", "toml", "ini"] and "ci" in component:
                return CommitType.CI
                
            if file_type in ["scss", "css", "less", "stylus"]:
                return CommitType.STYLE
        
        # Default to feat for most changes
        return CommitType.FEAT
    
    def _check_for_precommit_hooks(self) -> bool:
        """Check if the repository has pre-commit hooks configured."""
        try:
            with Spinner(message="Checking for pre-commit hooks...", spinner_type=1) as spinner:
                # Check for .pre-commit-config.yaml file
                config_file = os.path.join(self._get_git_root(), ".pre-commit-config.yaml")
                has_config = os.path.isfile(config_file)
                
                # Check for pre-commit hook in .git/hooks
                hooks_dir = os.path.join(self._get_git_root(), ".git", "hooks")
                pre_commit_hook = os.path.join(hooks_dir, "pre-commit")
                has_hook = os.path.isfile(pre_commit_hook) and os.access(pre_commit_hook, os.X_OK)
                
                # Check if pre-commit is installed
                spinner.update("Checking if pre-commit is installed...")
                try:
                    process = subprocess.Popen(
                        ["pre-commit", "--version"],
                        stdout=subprocess.PIPE,
                        stderr=subprocess.PIPE,
                        cwd=self.repo_path
                    )
                    process.communicate(timeout=2)
                    pre_commit_installed = process.returncode == 0
                except (subprocess.TimeoutExpired, FileNotFoundError):
                    pre_commit_installed = False
                
                # Log findings
                if has_config:
                    spinner.update("Found pre-commit configuration file")
                if has_hook:
                    spinner.update("Found pre-commit hook script")
                if pre_commit_installed:
                    spinner.update("Pre-commit is installed on system")
                
                result = has_config or has_hook or pre_commit_installed
                spinner.update(f"Pre-commit hooks {'detected' if result else 'not detected'}")
                return result
        except Exception as e:
            logger.debug(f"Error checking for pre-commit hooks: {str(e)}")
            return False
            
    def _is_precommit_module_available(self) -> bool:
        """Check if the pre-commit Python module is available in the current environment."""
        try:
            # Try to import pre_commit to check if it's available
            import importlib
            importlib.import_module('pre_commit')
            return True
        except ImportError:
            # Also check via subprocess as a fallback (for virtual environments)
            try:
                python_exe = sys.executable
                result = subprocess.run(
                    [python_exe, "-c", "import pre_commit; print('available')"],
                    stdout=subprocess.PIPE,
                    stderr=subprocess.PIPE,
                    timeout=2,
                    text=True
                )
                return result.returncode == 0 and "available" in result.stdout
            except (subprocess.SubprocessError, FileNotFoundError, OSError):
                return False
    
    def _revert_staged_changes(self) -> None:
        """Revert all staged changes to leave the repository in a clean state."""
        try:
            logger.info("Reverting staged changes...")
            # First try the modern 'git restore' command (Git 2.23+)
            stdout, code = self._run_git_command(["restore", "--staged", "."])
            
            # If that fails, fall back to the older 'git reset' command
            if code != 0:
                logger.debug("Modern 'git restore' failed, falling back to 'git reset'")
                _, reset_code = self._run_git_command(["reset", "HEAD", "."])
                if reset_code != 0:
                    logger.warning("Failed to revert staged changes. Repository may be in an inconsistent state.")
                    return
            
            logger.info("Successfully reverted all staged changes.")
        except Exception as e:
            logger.warning(f"Error while reverting staged changes: {str(e)}")
    
    def execute_commits(self, interactive: bool = True) -> None:
        """
        Execute the commits based on the analyzed groups.
        
        Args:
            interactive: Whether to run in interactive mode (prompting for confirmation)
        
        Raises:
            RuntimeError: If no commits were successful
        """
        if not self.commit_groups:
            print(f"\n{Colors.get_warning()}âš ï¸ No commit groups to process.{Colors.RESET}")
            return

        # Check for pre-commit hooks and auto-skip if module is not available
        if not self.skip_hooks and self._check_for_precommit_hooks() and not self._is_precommit_module_available():
            print(f"\n{Colors.get_warning()}âš ï¸ Pre-commit hooks detected but pre-commit module is not installed.{Colors.RESET}")
            print(f"{Colors.get_info()}Auto-enabling --skip-hooks to avoid commit errors.{Colors.RESET}")
            print(f"{Colors.get_info()}To use pre-commit hooks, install the module with: pip install pre-commit{Colors.RESET}")
            self.skip_hooks = True

        # Auto-skip hooks if hook validation fails
        if not self.skip_hooks and self._check_for_precommit_hooks():
            try:
                # Attempt to validate pre-commit hooks by running the command directly
                logger.debug("Running: pre-commit validate-config")
                result = subprocess.run(
                    ["pre-commit", "validate-config"],
                    capture_output=True,
                    text=True,
                    timeout=10, # Add a timeout
                    cwd=self.git_root # Run from git root
                )
                
                if result.returncode != 0:
                    print(f"\n{Colors.get_warning()}âš ï¸ Pre-commit hook validation failed.{Colors.RESET}")
                    logger.warning(f"pre-commit validate-config failed: {result.stderr or result.stdout}")
                    print(f"{Colors.get_info()}Auto-enabling --skip-hooks to avoid commit errors.{Colors.RESET}")
                    print(f"{Colors.get_info()}Fix your pre-commit configuration or ensure pre-commit is installed.{Colors.RESET}")
                    self.skip_hooks = True
                else:
                    logger.debug("Pre-commit configuration validated successfully.")
            except FileNotFoundError:
                # Handle case where pre-commit executable is not found
                logger.warning("'pre-commit' command not found. Skipping validation.")
                print(f"\n{Colors.get_warning()}âš ï¸ 'pre-commit' command not found. Cannot validate hooks.{Colors.RESET}")
                print(f"{Colors.get_info()}Install pre-commit or ensure it's in your PATH.{Colors.RESET}")
                # Consider skipping hooks if validation fails due to missing command
                # self.skip_hooks = True
            except subprocess.TimeoutExpired:
                logger.warning("pre-commit validate-config timed out. Skipping validation.")
                print(f"\n{Colors.get_warning()}âš ï¸ Pre-commit validation timed out. Skipping.{Colors.RESET}")
                self.skip_hooks = True
            except Exception as e:
                logger.warning(f"Error validating pre-commit hooks: {str(e)}")
                print(f"\n{Colors.get_warning()}âš ï¸ Failed to validate pre-commit hooks: {str(e)}{Colors.RESET}")
                print(f"{Colors.get_info()}Auto-enabling --skip-hooks to avoid potential commit errors.{Colors.RESET}")
                self.skip_hooks = True

        # Check for sensitive files
        self._check_and_report_sensitive_files()
        
        success_count = 0
        skip_count = 0
        
        for i, group in enumerate(self.commit_groups):
            print(f"\n{Colors.get_primary()}Commit {i+1}/{len(self.commit_groups)}: {group.name} ({group.file_count} files){Colors.RESET}")
            
            # Skip groups that only contain sensitive files if security scanning is enabled
            if self.security_scan and all(change.is_sensitive for change in group.changes):
                print(f"{Colors.get_warning()}âš ï¸ Skipping this group as all files contain sensitive data.{Colors.RESET}")
                skip_count += 1
                continue
                
            # Filter out sensitive files from this group
            group = self._filter_sensitive_files_from_group(group)
            
            # Skip empty groups
            if not group.changes:
                print(f"{Colors.get_warning()}âš ï¸ No safe files left to commit in this group. Skipping.{Colors.RESET}")
                skip_count += 1
                continue
            
            # Print files in this group
            self._print_files_in_group(group)
            
            # Generate the commit message
            commit_message = group.generate_commit_message()
            
            # In interactive mode, ask for confirmation
            if interactive and not self._confirm_commit(group, commit_message):
                skip_count += 1
                continue
            
            # Execute the commit
            if self._commit_changes(group, commit_message):
                success_count += 1
        
        # Check if all commits failed
        if success_count == 0 and self.commit_groups:
            self._handle_failed_commits(skip_count)

    def _check_and_report_sensitive_files(self) -> None:
        """Check for sensitive files and report them to the user."""
        sensitive_files = [change for change in self.changes if change.is_sensitive]
        if sensitive_files and self.security_scan:
            print(f"\n{Colors.get_warning()}âš ï¸ Security warning: {len(sensitive_files)} files have been flagged as potentially containing sensitive data.{Colors.RESET}")
            print(f"{Colors.get_info()}You will be prompted to confirm inclusion of each sensitive file during the commit process.{Colors.RESET}")
            print(f"{Colors.get_info()}Use --no-security flag to disable security scanning entirely if needed.{Colors.RESET}")
            for change in sensitive_files[:5]:  # Show only first 5 to avoid too much output
                print(f"  {Colors.get_warning()}{change.filename}{Colors.RESET}: {change.sensitive_reason}")
            if len(sensitive_files) > 5:
                print(f"  {Colors.get_warning()}...and {len(sensitive_files) - 5} more files{Colors.RESET}")
            print("")

    def _prompt_for_sensitive_file(self, change: GitChange) -> bool:
        """
        Prompt the user for confirmation to include a file flagged as sensitive.
        Implements double confirmation for security.
        
        Args:
            change: The GitChange object flagged as sensitive
            
        Returns:
            True if the user confirms to include the file, False otherwise
        """
        # Check if we're running in a TUI environment
        try:
            # Try to access global app instance to see if we're in TUI mode
            # This is a dirty but effective way to detect TUI mode
            import builtins
            tui_app = getattr(builtins, "_TEXTUAL_APP", None)
            
            if tui_app:
                # We're in TUI mode, use the TUI dialog
                from .tui import SensitiveFileConfirmScreen
                
                result = [False]  # Use a list to allow modification from callbacks
                
                def confirm_callback():
                    result[0] = True
                
                def cancel_callback():
                    result[0] = False
                
                # Create and configure the confirmation screen
                screen = SensitiveFileConfirmScreen()
                screen.change = change
                screen.on_confirm = confirm_callback
                screen.on_cancel = cancel_callback
                
                # Show the screen and wait for result
                # This is synchronous because TUI runs in event loop
                tui_app.push_screen(screen)
                tui_app._process_messages(wait_for_idle=True)
                
                return result[0]
        except (ImportError, AttributeError):
            # Not in TUI mode or error detecting, fall back to CLI
            pass
            
        # CLI mode confirmation
        print(f"\n{Colors.get_warning()}âš ï¸ SECURITY WARNING: {change.filename} was flagged as sensitive{Colors.RESET}")
        print(f"  {Colors.get_info()}Reason: {change.sensitive_reason}{Colors.RESET}")
        
        # First confirmation
        confirmation = input(f"{Colors.get_warning()}Are you SURE you want to include this potentially sensitive file? [y/N]: {Colors.RESET}")
        if confirmation.lower() not in ['y', 'yes']:
            return False
            
        # Second confirmation for extra security
        second_confirmation = input(f"{Colors.get_error()}âš ï¸ FINAL WARNING: Including sensitive data in commits can lead to security breaches.{Colors.RESET}\n{Colors.get_warning()}Type 'CONFIRM' to proceed anyway: {Colors.RESET}")
        if second_confirmation != "CONFIRM":
            print(f"{Colors.get_success()}âœ“ File will be excluded for safety.{Colors.RESET}")
            return False
            
        print(f"{Colors.get_warning()}âš ï¸ You have chosen to include a sensitive file in your commit.{Colors.RESET}")
        return True

    def _filter_sensitive_files_from_group(self, group: CommitGroup) -> CommitGroup:
        """
        Filter out sensitive files from a commit group, with interactive confirmation option.
        
        Args:
            group: The commit group to filter
            
        Returns:
            The filtered group (may be empty if all files were sensitive)
        """
        if not self.security_scan:
            return group
            
        original_count = len(group.changes)
        filtered_changes = []
        excluded_count = 0
        
        for change in group.changes:
            if not change.is_sensitive:
                filtered_changes.append(change)
            else:
                # Ask user if they want to include this sensitive file
                if self._prompt_for_sensitive_file(change):
                    # User confirmed they want to include this file despite the warning
                    # Mark it as safe by clearing the sensitive flag
                    change.is_sensitive = False
                    change.sensitive_reason = "Manually approved by user despite security warning"
                    filtered_changes.append(change)
                else:
                    excluded_count += 1
        
        # Update group with filtered changes
        group.changes = filtered_changes
        
        if excluded_count > 0:
            print(f"{Colors.get_warning()}âš ï¸ {excluded_count} sensitive files will be excluded from this commit.{Colors.RESET}")
        
        return group

    def _print_files_in_group(self, group: CommitGroup) -> None:
        """Print the files included in a commit group."""
        print(f"{Colors.get_info()}Files in this commit:{Colors.RESET}")
        for change in group.changes:
            # Determine symbol based on CLEANED status
            if change.status == "??": 
                status_symbol = "+"  # Use + for untracked
            elif change.status == "D": 
                status_symbol = "-"  # Use - for deleted
            else: 
                status_symbol = "M"  # Use M for modified, added, renamed, copied
            
            # Make sure filename doesn't start with status code
            clean_filename = change.filename
            if clean_filename.startswith(('M ', 'A ', 'D ', '?? ')):
                clean_filename = clean_filename[2:].strip()
            elif clean_filename.startswith(('M', 'A', 'D', '??')) and len(clean_filename) > 1:
                if clean_filename[1] != ' ':
                    clean_filename = clean_filename[1:].strip()
            
            # Print symbol and clean filename
            print(f"  {status_symbol} {clean_filename}") 
        print()

    def _confirm_commit(self, group: CommitGroup, commit_message: str) -> bool:
        """
        Ask for confirmation before committing.
        
        Args:
            group: The commit group to commit
            commit_message: The generated commit message
            
        Returns:
            True if the commit is confirmed, False otherwise
        """
        print(f"\n{Colors.get_info()}Commit message:{Colors.RESET}")
        print(f"{Colors.get_highlight()}{commit_message}{Colors.RESET}")
        
        response = input(f"\n{Colors.get_info()}Proceed with this commit? [Y/n/e] (Y=yes, n=skip, e=edit): {Colors.RESET}")
        
        if response.lower() == 'n':
            print(f"{Colors.get_warning()}Skipping this commit.{Colors.RESET}")
            return False
        elif response.lower() == 'e':
            # Allow editing the commit message
            return self._handle_commit_edit(group)
        
        return True

    def _handle_commit_edit(self, group: CommitGroup) -> bool:
        """
        Handle commit message editing.
        
        Args:
            group: The commit group to edit
            
        Returns:
            True if the commit should proceed, False to skip
        """
        # TODO: Implement commit message editing with a proper editor
        print(f"{Colors.get_warning()}Sorry, commit message editing is not yet implemented.{Colors.RESET}")
        
        # For now, ask to proceed with original message or skip
        response = input(f"{Colors.get_info()}Proceed with original commit message? [Y/n]: {Colors.RESET}")
        return response.lower() != 'n'

    def _commit_changes(self, group: CommitGroup, commit_message: str) -> bool:
        """
        Commit a group of changes with a given message.
        
        Args:
            group: The commit group containing files to commit
            commit_message: The commit message to use
            
        Returns:
            True if the commit was successful, False otherwise
        """
        try:
            # Validate file paths for debugging
            if logger.level <= logging.DEBUG:
                self._validate_file_paths(group)
                
            # Stage the files
            if not self._stage_files(group):
                return False
                
            # Check for pre-commit hooks activation
            needs_hooks = self._check_for_precommit_hooks()
            
            # Create the commit
            if needs_hooks and not self.skip_hooks:
                print(f"{Colors.get_info()}ðŸ“ Creating commit with pre-commit hooks...{Colors.RESET}")
                with tempfile.NamedTemporaryFile(mode='w', delete=False, encoding='utf-8') as f:
                    f.write(commit_message)
                    temp_path = f.name
                
                try:
                    # Try with hooks first
                    cmd = ["commit", "-F", temp_path]
                    out, exit_code = self._run_git_command(cmd)
                    
                    if exit_code != 0:
                        logger.warning(f"Commit failed with hooks: {out}")
                        print(f"{Colors.get_warning()}âš ï¸ Pre-commit hook may have failed: {out}{Colors.RESET}")
                        
                        # Try without hooks if the error might be hook-related
                        if "pre-commit" in out.lower() or self._is_precommit_module_available() == False:
                            return self._retry_commit_without_hooks(commit_message)
                        else:
                            print(f"{Colors.get_error()}âŒ Commit failed{Colors.RESET}")
                            return False
                    else:
                        print(f"{Colors.get_success()}âœ… Committed successfully{Colors.RESET}")
                        return True
                finally:
                    # Clean up temp file
                    try:
                        os.unlink(temp_path)
                    except:
                        pass
            else:
                # No hooks needed or skipping hooks
                print(f"{Colors.get_info()}ðŸ“ Creating commit...{Colors.RESET}")
                cmd = ["commit", "-m", commit_message]
                if self.skip_hooks or not needs_hooks:
                    cmd.append("--no-verify")
                
                out, exit_code = self._run_git_command(cmd)
                
                if exit_code != 0:
                    logger.warning(f"Commit failed: {out}")
                    print(f"{Colors.get_error()}âŒ Commit failed: {out}{Colors.RESET}")
                    return False
                else:
                    print(f"{Colors.get_success()}âœ… Committed successfully{Colors.RESET}")
                    return True
                    
        except Exception as e:
            logger.error(f"Error during commit: {e}")
            print(f"{Colors.get_error()}âŒ Error during commit: {str(e)}{Colors.RESET}")
            return False
    
    def _generate_ai_commit_message(self, group: CommitGroup) -> str:
        """Generate a commit message using AI or rule-based approaches."""
        logger.debug(f"Generating commit message for group: {group.name}")
        
        try:
            # Try to use a template first
            from .config import get_config
            config = get_config()
            
            try:
                # Try template-based approach first
                template = config.get_commit_template()
                logger.debug(f"Using commit template: {config.get('active_template')}")
                
                # Prepare variables for template
                component_str = "-".join(sorted({change.component for change in group.changes if change.component}))
                type_str = group.commit_type.value
                description = group.name
                
                # Prepare the body with change details
                body_lines = []
                if group.description:
                    body_lines.append(group.description)
                else:
                    body_lines.append(f"Update {len(group.changes)} files in {component_str}")
                
                # Format the files section
                files_lines = []
                for change in group.changes:
                    status_symbol = "+" if change.status == "??" else "M" if change.status == "M" else "D" if change.status == "D" else "R" if change.status == "R" else "?"
                    files_lines.append(f"- {status_symbol} {change.filename}")
                
                # Format issues references
                issues_str = "\n".join(f"Fixes #{issue}" for issue in sorted(group.issues)) if group.issues else ""
                
                # Prepare all template variables
                template_vars = {
                    "type": type_str,
                    "scope": component_str,
                    "description": description,
                    "body": "\n".join(body_lines),
                    "files": "\n".join(files_lines),
                    "issues": issues_str,
                    "breaking_change": ""  # Default empty
                }
                
                # Format the message parts
                subject = template["subject_template"].format(**template_vars)
                body = template["body_template"].format(**template_vars)
                footer = template["footer_template"].format(**template_vars) if issues_str or "breaking_change" in template_vars else ""
                
                # Combine all parts
                parts = [subject]
                if body:
                    parts.append(body)
                if footer:
                    parts.append(footer)
                
                return "\n\n".join(parts)
                
            except Exception as e:
                # If template approach fails, fall back to AI
                logger.warning(f"Template-based commit message failed, falling back to AI: {str(e)}")
                pass
                
            # AI-based approach
            if self.client and self.use_ai:
                logger.debug("Using AI to generate commit message")
                
                # Create prompt for commit message
                prompt = f"""Generate a single semantic git commit message for the following changes.

The commit is focused on: {group.name} ({group.commit_type.value})
Component/Area: {'-'.join(sorted({change.component for change in group.changes if change.component}))}

Files changed:
"""
                for change in group.changes:
                    prompt += f"- {change.filename} ({change.status})\n"
                
                prompt += "\nYour commit message should follow the Conventional Commits format:\n"
                prompt += "1. Start with type(scope): subject\n"
                prompt += "2. Then a blank line followed by a more detailed body\n"
                prompt += "3. Include an 'Affected files:' section with a list of changed files\n"
                
                if group.issues:
                    prompt += f"\nReference the following issues: {', '.join(sorted(group.issues))}\n"
                
                prompt += "\nCommit message:"
                
                # Generate message with AI
                system_prompt = """You are a helpful assistant that generates high-quality semantic git commit messages.
Follow the conventional commits format. Create concise but meaningful commit messages.
Include a relevant type and scope. The subject line should be no longer than 50 characters.
Include a more detailed body that explains what was changed and why.
End with a list of affected files and reference any issue numbers mentioned."""
                
                message = self.client.generate(prompt, system_prompt)
                if message:
                    # Clean up the message: remove any extra quotes, comments or AI-explanations
                    message = message.strip()
                    if message.startswith('```') and message.endswith('```'):
                        message = message[3:-3].strip()
                    if message.startswith('"') and message.endswith('"'):
                        message = message[1:-1].strip()
                    
                    logger.debug("Successfully generated AI commit message")
                    return message
        
        except Exception as e:
            logger.warning(f"Failed to generate AI commit message: {str(e)}")
        
        # Fallback to rule-based conventional message
        logger.debug("Falling back to rule-based commit message")
        
        # Determine scope from components
        components = {change.component for change in group.changes}
        scope = "-".join(sorted(list(components)[:2])) if components else "general"
        
        # Create subject line (first line of commit)
        # Limit to 50 characters for GitHub compatibility
        max_subject_length = 50
        
        # Start with type and scope
        prefix = f"{group.commit_type.value}({scope}): "
        available_chars = max_subject_length - len(prefix)
        
        # Ensure name is not longer than available chars
        name = group.name if len(group.name) <= available_chars else group.name[:available_chars]
        
        # Combine to form subject
        subject = f"{prefix}{name}"
        
        # Create body with file list
        body = group.description if group.description else f"Update {len(group.changes)} files in {scope}"
        
        # Add affected files as bullet points
        files_section = "\nAffected files:"
        for change in group.changes:
            status_symbol = "+" if change.status == "??" else "M" if change.status == "M" else "D" if change.status == "D" else "R"
            files_section += f"\n- {status_symbol} {change.filename}"
            
        # Add footer with issue references
        footer = ""
        if group.issues:
            footer = "\n\n" + "\n".join(f"Fixes #{issue}" for issue in sorted(group.issues))
            
        # Combine all parts
        return f"{subject}\n\n{body}{files_section}{footer}"

    def _stage_files(self, group: CommitGroup) -> bool:
        """
        Stage all files in a commit group.
        
        Args:
            group: The commit group containing files to stage
            
        Returns:
            True if staging was successful, False otherwise
        """
        try:
            with Spinner(message="Staging files...", spinner_type=2, show_progress_bar=True, total=len(group.changes)) as spinner:
                # Iterate using index to re-access the change object reliably
                for i in range(len(group.changes)):
                    # Get the change object directly from the group list
                    change_to_stage = group.changes[i]
                    
                    # The filename in the GitChange object should already be clean, but let's make absolutely sure
                    # by removing any potential status prefix (M, A, D, ??) that might have been incorrectly stored
                    filename_to_stage = change_to_stage.filename
                    
                    # Clean any leading status prefix that might have gotten stored with the filename
                    if filename_to_stage.startswith(('M ', 'A ', 'D ', '?? ')):
                        filename_to_stage = filename_to_stage[2:].strip()
                    elif filename_to_stage.startswith(('M', 'A', 'D', '??')) and len(filename_to_stage) > 1:
                        # Check if it's just the status code without space
                        if filename_to_stage[1] != ' ':  # Only trim if not followed by space (which would be part of the filename)
                            filename_to_stage = filename_to_stage[1:].strip()
                    
                    spinner.update(f"Staging {filename_to_stage}...", progress=i)
                    
                    if not filename_to_stage:
                        logger.warning(f"Skipping staging for empty filename derived from change index {i}: {change_to_stage!r}")
                        continue

                    # --- Add detailed logging for filename --- 
                    logger.debug(f"  Preparing to stage: original={repr(change_to_stage.filename)}, cleaned={repr(filename_to_stage)}")
                    # --- End logging --- 

                    # Determine correct git command arguments based on file status
                    # --- Compare against CLEANED status codes --- 
                    if change_to_stage.status == 'D':  # Deleted file status 
                        args = ["rm", "--", filename_to_stage] 
                    else:  # For Added (A), Modified (M), Renamed (R), Copied (C), Untracked (??)
                        args = ["add", "--", filename_to_stage]
                    
                    # Execute the command (self._run_git_command prepends 'git')
                    logger.debug(f"Running staging command via _run_git_command: args={args}")
                    out, exit_code = self._run_git_command(args)
                    
                    if exit_code != 0:
                        spinner.stop()
                        # Use filename_to_stage in log message
                        logger.warning(f"Failed to stage {filename_to_stage}: {out}") 
                        print(f"{Colors.get_error()}âŒ Failed to stage {filename_to_stage}: {out}{Colors.RESET}")
                        return False
                
                spinner.update("All files staged successfully", progress=len(group.changes))
                return True
        except Exception as e:
            logger.error(f"Error staging files: {e}")
            print(f"{Colors.get_error()}âŒ Error staging files: {str(e)}{Colors.RESET}")
            return False

    def _retry_commit_without_hooks(self, commit_message: str) -> bool:
        """
        Retry a commit without hooks if it failed due to pre-commit issues.
        
        Args:
            commit_message: The commit message to use
            
        Returns:
            True if the retry was successful, False otherwise
        """
        print(f"\n{Colors.get_warning()}âš ï¸ Pre-commit hook error detected. Trying again with --no-verify...{Colors.RESET}")
        
        # Try again with --no-verify
        commit_cmd = ["git", "commit", "-m", commit_message, "--no-verify"]
        out, exit_code = self._run_git_command(commit_cmd)
        
        if exit_code == 0:
            print(f"{Colors.get_success()}âœ… Committed (with hooks disabled){Colors.RESET}")
            return True
        else:
            logger.error(f"Commit still failed with --no-verify: {out}")
            print(f"{Colors.get_error()}âŒ Commit failed even with hooks disabled{Colors.RESET}")
            return False

    def _handle_failed_commits(self, skip_count: int) -> None:
        """
        Handle the case where all commits have failed.
        
        Args:
            skip_count: The number of commits that were skipped
            
        Raises:
            RuntimeError: Always, to indicate failure
        """
        if skip_count > 0:
            logger.error("All commits failed or were skipped.")
            print(f"\n{Colors.get_error()}âŒ No commits were made. {skip_count} were skipped by user or due to sensitive content.{Colors.RESET}")
        else:
            logger.error("All commits failed. This may be due to Git configuration issues.")
            print(f"\n{Colors.get_error()}âŒ All commits failed. This may be due to Git configuration issues.{Colors.RESET}")
            print(f"   To fix this issue, try one of the following:")
            print(f"   1. {Colors.get_info()}Check your Git configuration: git config --list{Colors.RESET}")
            print(f"   2. {Colors.get_info()}Ensure your user.name and user.email are set: git config --global user.name 'Your Name'{Colors.RESET}")
            print(f"   3. {Colors.get_info()}Try with hooks disabled: --skip-hooks{Colors.RESET}")
        
        raise RuntimeError("No commits were successful. Check the logs for details.")

    # New helper methods for fetching change details
    def _fetch_change_details_parallel(self) -> None:
        """Fetch diffs, content, and security scans in parallel."""
        logger.debug(f"Fetching change details in parallel with {self.resources['threads_available']} threads")
        max_threads = min(len(self.changes), self.resources["threads_available"])
        
        def process_change(change):
            self._get_diff_content_and_scan(change)

        with concurrent.futures.ThreadPoolExecutor(max_workers=max_threads) as executor:
            futures = [executor.submit(process_change, change) for change in self.changes]
            try:
                # Add a reasonable timeout based on number of changes
                timeout = max(60, len(self.changes) * 2) 
                concurrent.futures.wait(futures, timeout=timeout)
            except Exception as e:
                logger.warning(f"Error waiting for change detail processing pool: {str(e)}")

    def _fetch_change_details_sequential(self) -> None:
        """Fetch diffs, content, and security scans sequentially."""
        logger.debug("Fetching change details sequentially")
        for change in self.changes:
            self._get_diff_content_and_scan(change)

    def _get_diff_content_and_scan(self, change: GitChange) -> None:
        """Helper to get diff/content and perform security scan for a single change."""
        relative_filename = change.filename # Already relative from load_changes
        
        try:
            absolute_path = os.path.join(self.git_root, relative_filename)
            if os.path.exists(absolute_path):
                if change.status != "??":  # Not an untracked file
                    # Check staged first
                    diff_output, diff_code = self._run_git_command(["diff", "--cached", "--", relative_filename])
                    if diff_code != 0 or not diff_output:
                        diff_output, diff_code = self._run_git_command(["diff", "--", relative_filename])
                    
                    if diff_code == 0 and diff_output:
                        change.content_diff = diff_output
                else:
                    # Untracked file content
                    try:
                        with open(absolute_path, 'r', encoding='utf-8', errors='replace') as f:
                            content = f.read(50000)
                            change.content_diff = f"diff --git a/{relative_filename} b/{relative_filename}\n" + \
                                                f"new file mode 100644\n" + \
                                                f"--- /dev/null\n" + \
                                                f"+++ b/{relative_filename}\n" + \
                                                "".join([f"+{line}\n" for line in content.splitlines()])
                    except Exception as file_error:
                        logger.warning(f"Failed to read content of untracked file {relative_filename}: {str(file_error)}")

                # Security scan
                if self.security_scan and self.security_scanner:
                    file_content = None
                    try:
                        with open(absolute_path, 'r', encoding='utf-8', errors='replace') as f:
                            file_content = f.read(100000)
                    except Exception:
                        pass 
                    is_sensitive, reason = self.security_scanner.scan_file(relative_filename, file_content)
                    if is_sensitive:
                        change.is_sensitive = True
                        change.sensitive_reason = reason
                        logger.warning(f"Security scan flagged {relative_filename}: {reason}")
            else:
                logger.debug(f"Path does not exist for diff/scan: {absolute_path}")
        except Exception as e:
            logger.warning(f"Failed processing details for {relative_filename}: {str(e)}")

    def _print_files_in_group(self, group: CommitGroup) -> None:
        """Print the files included in a commit group."""
        print(f"{Colors.get_info()}Files in this commit:{Colors.RESET}")
        for change in group.changes:
            # Determine symbol based on CLEANED status
            if change.status == "??": 
                status_symbol = "+"  # Use + for untracked
            elif change.status == "D": 
                status_symbol = "-"  # Use - for deleted
            else: 
                status_symbol = "M"  # Use M for modified, added, renamed, copied
            
            # Make sure filename doesn't start with status code
            clean_filename = change.filename
            if clean_filename.startswith(('M ', 'A ', 'D ', '?? ')):
                clean_filename = clean_filename[2:].strip()
            elif clean_filename.startswith(('M', 'A', 'D', '??')) and len(clean_filename) > 1:
                if clean_filename[1] != ' ':
                    clean_filename = clean_filename[1:].strip()
            
            # Print symbol and clean filename
            print(f"  {status_symbol} {clean_filename}") 
        print()

    def _validate_file_paths(self, group: CommitGroup) -> None:
        """
        Validate file paths in a group to ensure they exist and are clean.
        This is a diagnostic method to help detect path issues.
        
        Args:
            group: The commit group containing files to validate
        """
        logger.debug("Validating file paths in commit group...")
        
        for change in group.changes:
            # Check if the filename has status prefixes
            has_status_prefix = False
            if change.filename.startswith(('M ', 'A ', 'D ', '?? ')):
                has_status_prefix = True
                clean_path = change.filename[2:].strip()
            elif change.filename.startswith(('M', 'A', 'D', '??')) and len(change.filename) > 1:
                if change.filename[1] != ' ':
                    has_status_prefix = True
                    clean_path = change.filename[1:].strip()
            else:
                clean_path = change.filename
            
            # Check if file exists in git root
            full_path = os.path.join(self.git_root, clean_path)
            exists = os.path.exists(full_path)
            
            # Log the results
            if has_status_prefix:
                logger.warning(f"File path has status prefix: '{change.filename}' -> cleaned: '{clean_path}'")
            
            if not exists and change.status != 'D':  # Deleted files won't exist
                logger.warning(f"File path does not exist: '{full_path}' (status: {change.status})")
            else:
                logger.debug(f"File path validation: '{clean_path}' exists={exists}, status={change.status}")
                
        logger.debug("File path validation complete")


def display_banner(use_color=True):
    """Display a cool banner for the tool."""
    if use_color:
        primary = Colors.get_primary()
        secondary = Colors.get_secondary()
        accent = Colors.get_accent()
        highlight = Colors.get_highlight()
        reset = Colors.RESET
    else:
        primary = secondary = accent = highlight = reset = ""
    
    version = get_version()
    
    # Set fixed width for banner to ensure alignment
    banner_width = 130
    
    if Colors.THEME == "cyberpunk":
        # Cyberpunk-themed ASCII art banner
        banner_lines = [
            f"{primary} .--..--..--..--..--..--..--..--..--..--..--..--..--..--..--..--..--..--..--..--..--..--..--..--..--..--..--..--..--..--..--..--..--..--..--. {reset}",
            f"{primary}/ .. \\.. \\.. \\.. \\.. \\.. \\.. \\.. \\.. \\.. \\.. \\.. \\.. \\.. \\.. \\.. \\.. \\.. \\.. \\.. \\.. \\.. \\.. \\.. \\.. \\.. \\.. \\.. \\.. \\.. \\.. \\.. \\.. \\.. \\.. \\{reset}",
            f"{primary}\\ \\/\\ `'\\ `'\\ `'\\ `'\\ `'\\ `'\\ `'\\ `'\\ `'\\ `'\\ `'\\ `'\\ `'\\ `'\\ `'\\ `'\\ `'\\ `'\\ `'\\ `'\\ `'\\ `'\\ `'\\ `'\\ `'\\ `'\\ `'\\ `'\\ `'\\ `'\\ `'\\ `'\\ `'\\ \\/ /{reset}",
            f"{primary} \\/ /`--'`--'`--'`--'`--'`--'`--'`--'`--'`--'`--'`--'`--'`--'`--'`--'`--'`--'`--'`--'`--'`--'`--'`--'`--'`--'`--'`--'`--'`--'`--'`--'`--'\\/ / {reset}",
            f"{primary} / /\\    {secondary}____                                __          ____        __          ____                                        __{primary}          / /\\ {reset}",
            f"{primary}/ /\\ \\  {secondary}/\\  _`\\                             /\\ \\__      /\\  _`\\   __/\\ \\__      /\\  _`\\                                   __/\\ \\__{primary}      / /\\ \\{reset}",
            f"{primary}\\ \\/ /  {secondary}\\ \\,\\L\\_\\    ___ ___      __    _ __\\ \\ ,_\\     \\ \\ \\L\\_\\/\\_\\ \\ ,_\\     \\ \\ \\/\\_\\    ___     ___ ___     ___ ___/\\_\\ \\ ,_\\{primary}     \\ \\/ /{reset}",
            f"{primary} \\/ /    {secondary}\\/_\\__ \\  /' __` __`\\  /'__`\\ /\\`'__\\ \\ \\/      \\ \\ \\L_L\\/\\ \\ \\ \\/      \\ \\ \\/_/_  / __`\\ /' __` __`\\ /' __` __`\\/\\ \\ \\ \\/{primary}      \\/ / {reset}",
            f"{primary} / /\\      {secondary}/\\ \\L\\ \\/\\ \\/\\ \\/\\ \\/\\ \\L\\.\\\\ \\ \\/ \\ \\ \\_      \\ \\ \\/, \\ \\ \\ \\ \\_      \\ \\ \\L\\ \\/\\ \\L\\ \\/\\ \\/\\ \\/\\ \\/\\ \\/\\ \\/\\ \\ \\ \\ \\ \\__{primary}     / /\\ {reset}",
            f"{primary}/ /\\ \\     {secondary}\\ `\\____\\ \\_\\ \\_\\ \\_\\ \\__/.\\_\\ \\_\\  \\ \\__\\      \\ \\____/\\ \\_\\ \\__\\      \\ \\____/\\ \\____/\\ \\_\\ \\_\\ \\_\\ \\_\\ \\_\\ \\_\\ \\_\\ \\__\\{primary}   / /\\ \\{reset}",
            f"{primary}\\ \\/ /      {secondary}\\/_____/\\/_/\\/_/\\/_/\\/__/\\/_/\\/_/   \\/__/       \\/___/  \\/_/\\/__/       \\/___/  \\/___/  \\/_/\\/_/\\/_/\\/_/\\/_/\\/_/\\/_/\\/__/{primary}   \\ \\/ /{reset}",
            f"{primary} \\/ /                                                                                                                                    \\/ / {reset}",
            f"{accent} / /\\.--..--..--..--..--..--..--..--..--..--..--..--..--..--..--..--..--..--..--..--..--..--..--..--..--..--..--..--..--..--..--..--..--./ /\\ {reset}",
            f"{accent}/ /\\ \\.. \\.. \\.. \\.. \\.. \\.. \\.. \\.. \\.. \\.. \\.. \\.. \\.. \\.. \\.. \\.. \\.. \\.. \\.. \\.. \\.. \\.. \\.. \\.. \\.. \\.. \\.. \\.. \\.. \\.. \\.. \\.. \\.. \\/\\ \\{reset}",
            f"{accent}\\ `'\\ `'\\ `'\\ `'\\ `'\\ `'\\ `'\\ `'\\ `'\\ `'\\ `'\\ `'\\ `'\\ `'\\ `'\\ `'\\ `'\\ `'\\ `'\\ `'\\ `'\\ `'\\ `'\\ `'\\ `'\\ `'\\ `'\\ `'\\ `'\\ `'\\ `'\\ `'\\ `'\\ `' /{reset}",
            f"{accent} `--'`--'`--'`--'`--'`--'`--'`--'`--'`--'`--'`--'`--'`--'`--'`--'`--'`--'`--'`--'`--'`--'`--'`--'`--'`--'`--'`--'`--'`--'`--'`--'`--'`--'`--' {reset}",
            f"{reset}        {primary}Smart Git Commit v{version}{reset}"
        ]
    elif Colors.THEME == "dracula":
        # Dracula-themed banner with gothic style
        banner_lines = [
            f"{primary}   â–„â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ{reset}  {highlight}  â–„â–„â–„â–„â–ˆâ–ˆâ–ˆâ–„â–„â–„â–„      â–„â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ{reset}    {secondary}  â–„â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     â–ˆâ–ˆâ–ˆ{reset}    {primary} â–„â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  â–„â–ˆ      â–ˆâ–ˆâ–ˆ{reset} ",
            f"{primary}  â–ˆâ–ˆâ–ˆ    â–ˆâ–ˆâ–ˆ{reset} {highlight}â–„â–ˆâ–ˆâ–€â–€â–€â–ˆâ–ˆâ–ˆâ–€â–€â–€â–ˆâ–ˆâ–„   â–ˆâ–ˆâ–ˆ    â–ˆâ–ˆâ–ˆ{reset}    {secondary} â–ˆâ–ˆâ–ˆ    â–ˆâ–ˆâ–ˆ â–€â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„{reset} {primary}â–ˆâ–ˆâ–ˆ    â–ˆâ–ˆâ–ˆ â–ˆâ–ˆâ–ˆ  â–€â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„{reset}",
            f"{primary}  â–ˆâ–ˆâ–ˆ    â–ˆâ–€{reset}  {highlight}â–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆ    â–ˆâ–ˆâ–ˆ{reset}    {secondary} â–ˆâ–ˆâ–ˆ    â–ˆâ–€     â–€â–ˆâ–ˆâ–ˆâ–€â–€â–ˆâ–ˆ{reset} {primary}â–ˆâ–ˆâ–ˆ    â–ˆâ–ˆâ–ˆ â–ˆâ–ˆâ–ˆâ–Œ    â–€â–ˆâ–ˆâ–ˆâ–€â–€â–ˆâ–ˆ{reset}",
            f"{primary}  â–ˆâ–ˆâ–ˆ{reset}       {highlight}â–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆ    â–ˆâ–ˆâ–ˆ{reset}    {secondary} â–ˆâ–ˆâ–ˆ           â–ˆâ–ˆâ–ˆ   â–€{reset} {primary}â–ˆâ–ˆâ–ˆ    â–ˆâ–ˆâ–ˆ â–ˆâ–ˆâ–ˆâ–Œ     â–ˆâ–ˆâ–ˆ   â–€{reset}",
            f"{accent}â–€â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ{reset} {highlight}â–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆ â–€â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ{reset}    {secondary} â–ˆâ–ˆâ–ˆ         â–„â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ{reset}{primary}â–€â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ â–ˆâ–ˆâ–ˆâ–Œ     â–ˆâ–ˆâ–ˆ{reset}",
            f"{accent}         â–ˆâ–ˆâ–ˆ{reset} {highlight}â–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆ    â–ˆâ–ˆâ–ˆ{reset}    {secondary} â–ˆâ–ˆâ–ˆ    â–ˆâ–„  â–ˆâ–ˆâ–ˆ    â–ˆâ–ˆâ–ˆ{reset}    {primary}   â–ˆâ–ˆâ–ˆ â–ˆâ–ˆâ–ˆ      â–ˆâ–ˆâ–ˆ{reset}",
            f"{accent}   â–„â–ˆ    â–ˆâ–ˆâ–ˆ{reset} {highlight}â–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆ    â–ˆâ–ˆâ–ˆ{reset}    {secondary} â–ˆâ–ˆâ–ˆ    â–ˆâ–ˆâ–ˆ â–ˆâ–ˆâ–ˆ    â–ˆâ–ˆâ–ˆ{reset}    {primary}   â–ˆâ–ˆâ–ˆ â–ˆâ–ˆâ–ˆ      â–ˆâ–ˆâ–ˆ{reset}",
            f"{accent} â–„â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–€{reset}  {highlight} â–€â–ˆ   â–ˆâ–ˆâ–ˆ   â–ˆâ–€    â–ˆâ–ˆâ–ˆ    â–ˆâ–€{reset}     {secondary} â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–€   â–€â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–€{reset}     {primary}   â–ˆâ–€  â–ˆâ–€      â–„â–ˆâ–ˆâ–ˆâ–ˆâ–€{reset}",
            f"{reset}                                                                                     ",
            f"{reset}        {primary}Smart Git Commit v{version}{reset}"
        ]
    elif Colors.THEME == "nord":
        # Nord-themed banner with icy style
        banner_lines = [
            f"{primary}  â–„â–€â–€â–€ â–ˆâ–„ â–„â–ˆ â–„â–€â–€â–„ â–ˆâ–€â–€â–„ â–€â–ˆâ–€{reset}    {secondary}  â–„â–€â–€â–„ â–ˆ â–€â–ˆâ–€{reset}    {accent}  â–„â–€â–€â–€ â–„â–€â–€â–„ â–ˆâ–„ â–„â–ˆ â–ˆâ–„ â–„â–ˆ â–ˆ â–€â–ˆâ–€{reset}",
            f"{primary} â–€â–€â–€â–ˆ â–ˆ â–ˆ â–ˆ â–ˆâ–„â–„â–ˆ â–ˆâ–„â–„â–€  â–ˆ {reset}    {secondary}  â–ˆ    â–ˆ  â–ˆ {reset}    {accent}  â–ˆ    â–ˆ  â–ˆ â–ˆ â–ˆ â–ˆ â–ˆ â–ˆ â–ˆ  â–ˆ {reset}",
            f"{primary} â–€â–€â–€  â–ˆ   â–ˆ â–ˆ  â–ˆ â–€  â–€â–„ â–€ {reset}    {secondary}  â–€â–„â–„â–€ â–€ â–€â–€â–€{reset}    {accent}  â–€â–„â–„â–€ â–€â–„â–„â–€ â–ˆ   â–ˆ â–ˆ   â–ˆ â–€ â–€â–€â–€{reset}",
            f"{reset}                                                                     ",
            f"{reset}               {highlight}â•”â•â•—â”¬ â”¬â”Œâ”€â”â”¬â”€â”â”Œâ”¬â”  â•”â•â•—â”¬â”Œâ”¬â”  â•”â•â•—â”Œâ”€â”â”Œâ”¬â”â”Œâ”¬â”â”¬â”Œâ”¬â”{reset}",
            f"{reset}               {highlight}â•šâ•â•—â”‚ â”‚â”‚ â”‚â”œâ”¬â”˜ â”‚   â•‘ â•¦â”‚ â”‚   â•‘  â”‚ â”‚â”‚â”‚â”‚â”‚â”‚â”‚ â”‚ {reset}",
            f"{reset}               {highlight}â•šâ•â•â””â”€â”˜â””â”€â”˜â”´â””â”€ â”´   â•šâ•â•â”´ â”´   â•šâ•â•â””â”€â”˜â”´ â”´â”´ â”´ â”´ {reset}",
            f"{reset}                                                                     ",
            f"{reset}        {primary}Smart Git Commit v{version}{reset}"
        ]
    elif Colors.THEME == "monokai":
        # Monokai-themed ASCII art banner
        banner_lines = [
            f"{primary}    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—{reset}{secondary}     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—{reset}",
            f"{primary}    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â•šâ•â•â–ˆâ–ˆâ•”â•â•â•{reset}{secondary}    â–ˆâ–ˆâ•”â•â•â•â•â• â–ˆâ–ˆâ•‘â•šâ•â•â–ˆâ–ˆâ•”â•â•â•{reset}",
            f"{primary}    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•   â–ˆâ–ˆâ•‘   {reset}{secondary}    â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘   {reset}",
            f"{primary}    â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—   â–ˆâ–ˆâ•‘   {reset}{secondary}    â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘   {reset}",
            f"{primary}    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘   {reset}{secondary}    â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘   {reset}",
            f"{primary}    â•šâ•â•â•â•â•â•â•â•šâ•â•     â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•  â•šâ•â•   â•šâ•â•   {reset}{secondary}     â•šâ•â•â•â•â•â• â•šâ•â•   â•šâ•â•   {reset}",
            f"{accent}    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—{reset}",
            f"{accent}   â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ•â•â–ˆâ–ˆâ•”â•â•â•{reset}",
            f"{accent}   â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘   {reset}",
            f"{accent}   â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘   {reset}",
            f"{accent}   â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘   {reset}",
            f"{accent}    â•šâ•â•â•â•â•â• â•šâ•â•â•â•â•â• â•šâ•â•     â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•   â•šâ•â•   {reset}",
            f"{highlight}    â˜…å½¡[á´á´É´á´á´‹á´€Éª á´‡á´…Éªá´›Éªá´É´]å½¡â˜…{reset}",
            f"{reset}        {primary}Smart Git Commit v{version}{reset}"
        ]
    else:
        # Standard banner with improved alignment
        banner_lines = [
            f"{primary}  _____  __  __            _____  _______    _____ _____ _______    _____ ____  __  __ __  __ _____ _______ {reset}",
            f"{primary} / ____||  \\/  |    /\\    |  __ \\|__   __|  / ____|_   _|__   __|  / ____|___ \\|  \\/  |  \\/  |_   _|__   __|{reset}",
            f"{secondary}| (___  | \\  / |   /  \\   | |__) |  | |    | |  __  | |    | |    | |      __) | \\  / | \\  / | | |    | |   {reset}",
            f"{secondary} \\___ \\ | |\\/| |  / /\\ \\  |  _  /   | |    | | |_ | | |    | |    | |     |__ <| |\\/| | |\\/| | | |    | |   {reset}",
            f"{primary} ____) || |  | | / ____ \\ | | \\ \\   | |    | |__| |_| |_   | |    | |____ ___) | |  | | |  | |_| |_   | |   {reset}",
            f"{primary}|_____/ |_|  |_|/_/    \\_\\|_|  \\_\\  |_|     \\_____|_____|  |_|     \\_____|____/|_|  |_|_|  |_|_____|  |_|   {reset}",
            f"{reset}",
            f"{primary}                                 Smart Git Commit v{version}{reset}"
        ]
    
    # Print each line of the banner
    print("")  # Start with a blank line
    for line in banner_lines:
        print(line)
    print("")  # Add spacing
    
    # Create box with fixed width
    box_width = 60
    
    # Top border
    print(f"{accent}â•”{'â•' * (box_width - 2)}â•—{reset}")
    
    # Message
    message = f" {primary}AI-powered Git workflow{reset} for {secondary}intelligent{reset} commit messages "
    padding = box_width - len(message.replace(primary, "").replace(secondary, "").replace(reset, "")) - 2
    left_padding = padding // 2
    right_padding = padding - left_padding
    print(f"{accent}â•‘{' ' * left_padding}{primary}AI-powered Git workflow{reset} for {secondary}intelligent{reset} commit messages{' ' * right_padding}{accent}â•‘{reset}")
    
    # Bottom border
    print(f"{accent}â•š{'â•' * (box_width - 2)}â•{reset}")
    print("")  # End with a blank line


def print_section_header(title, use_color=True):
    """Display a styled section header."""
    if use_color:
        primary = Colors.get_primary()
        secondary = Colors.get_secondary()
        accent = Colors.get_accent()
        highlight = Colors.get_highlight()
        reset = Colors.RESET
    else:
        primary = secondary = accent = highlight = reset = ""
    
    # Fixed width for all section headers
    width = 60
    
    print("")  # Add spacing before section header
    
    if Colors.THEME == "cyberpunk":
        # Cyberpunk-themed section header with box drawing characters
        print(f"{accent}â”â”â”â”â”«{primary} {title} {accent}â”£{'â”' * (width - len(title) - 10)}â”“{reset}")
        print(f"{accent}â”—{'â”' * (width - 2)}â”›{reset}")
    elif Colors.THEME == "dracula":
        # Dracula-themed section header with gothic style
        print(f"{primary}â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„{reset}")
        print(f"{accent}â–ˆâ–ˆ {highlight}{title} {accent}{'â–ˆ' * (width - len(title) - 6)}â–ˆâ–ˆ{reset}")
        print(f"{primary}â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€{reset}")
    elif Colors.THEME == "nord":
        # Nord-themed section header with icy style
        print(f"{primary}â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€{secondary}[ {title} ]{primary}{'â”€' * (width - len(title) - 12)}â•®{reset}")
        print(f"{primary}â•°{'â”€' * (width - 2)}â•¯{reset}")
    elif Colors.THEME == "monokai":
        # Monokai-themed section header with vibrant style
        print(f"{primary}â•”â•â•{highlight}â˜… {title} â˜…{primary}{'â•' * (width - len(title) - 8)}â•—{reset}")
        print(f"{secondary}â•š{'â•' * (width - 2)}â•{reset}")
    else:
        # Standard theme section header
        print(f"{primary}â•â•â•â•â•[ {title} ]{'â•' * (width - len(title) - 10)}{reset}")
    
    print("")  # Add spacing after section header


def parse_args():
    """Parse command line arguments."""
    parser = argparse.ArgumentParser(
        description="Smart Git Commit - Generate and commit meaningful git commits",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    
    # Theme and appearance options
    appearance_group = parser.add_argument_group("Appearance Options")
    appearance_group.add_argument(
        "--theme", 
        choices=["standard", "cyberpunk", "dracula", "nord", "monokai"], 
        default="cyberpunk",
        help="Set the color theme for the CLI"
    )
    appearance_group.add_argument(
        "--no-color", 
        action="store_true", 
        help="Disable colored output"
    )
    
    # AI options
    ai_group = parser.add_argument_group("AI Options")
    ai_group.add_argument("--no-ai", action="store_true", help="Disable AI-powered analysis")
    ai_group.add_argument("--ai-provider", choices=["ollama", "openai"], help="AI provider to use (default: ollama)", default="ollama")
    ai_group.add_argument("--ollama-host", help="Host for Ollama API", default="http://localhost:11434")
    ai_group.add_argument("--ollama-model", help="Model to use for Ollama (will prompt if not specified)")
    ai_group.add_argument("--openai-api-key", help="OpenAI API key (can also be set with OPENAI_API_KEY environment variable)")
    ai_group.add_argument("--openai-model", help="Model to use for OpenAI", default="gpt-3.5-turbo")
    
    # Main commands
    parser.add_argument("--repo-path", help="Path to the git repository", default=".")
    parser.add_argument("--non-interactive", action="store_true", help="Run without interactive prompts")
    parser.add_argument("--analyze-only", action="store_true", help="Only analyze and group changes, don't commit")
    parser.add_argument("--tui", action="store_true", help="Launch the text-based user interface")
    parser.add_argument("--timeout", type=int, help=f"Timeout in seconds for HTTP requests (default: {DEFAULT_TIMEOUT})", default=DEFAULT_TIMEOUT)
    parser.add_argument("--verbose", action="store_true", help="Show verbose debug output")
    parser.add_argument("--skip-hooks", action="store_true", help="Skip Git hooks when committing (useful if pre-commit is not installed)")
    parser.add_argument("--no-revert", action="store_true", help="Don't automatically revert staged changes on error")
    parser.add_argument("--no-parallel", action="store_true", help="Disable parallel processing for slower but more stable operation")
    parser.add_argument("--no-security", action="store_true", help="Disable security layer that detects sensitive data in commits")
    parser.add_argument("--version", action="store_true", help="Show version information and support links")
    
    # Create subparsers for different commands
    subparsers = parser.add_subparsers(dest="command", help="Command to run")
    
    # Squash command
    squash_parser = subparsers.add_parser("squash", help="Squash related commits for a cleaner history")
    squash_parser.add_argument(
        "--strategy", 
        choices=["auto", "related_files", "same_author", "time_window", "semantic", "conventional"],
        default="auto",
        help="Strategy to use for identifying squashable commits"
    )
    squash_parser.add_argument(
        "--count", 
        type=int, 
        default=10,
        help="Number of recent commits to analyze"
    )
    squash_parser.add_argument(
        "--similarity-threshold", 
        type=float, 
        default=0.6,
        help="Similarity threshold for related files (0.0-1.0)"
    )
    squash_parser.add_argument(
        "--time-window", 
        type=int, 
        default=60,
        help="Time window in minutes for time-window strategy"
    )
    squash_parser.add_argument(
        "--no-interactive", 
        action="store_true",
        help="Run in non-interactive mode"
    )
    squash_parser.add_argument(
        "--auto-squash", 
        action="store_true",
        help="Automatically squash without confirmation"
    )
    
    # Hook command
    hook_parser = subparsers.add_parser("hook", help="Run as a Git hook")
    hook_parser.add_argument(
        "hook_type", 
        choices=["pre-commit", "pre-push", "post-merge", "post-checkout", "post-rewrite"],
        help="Type of Git hook"
    )
    hook_parser.add_argument(
        "args", 
        nargs="*", 
        help="Additional arguments to pass to the hook"
    )
    
    # Hook setup commands
    setup_parser = subparsers.add_parser("setup-hooks", help="Set up Git hooks")
    setup_parser.add_argument(
        "--hook-types", 
        nargs="+", 
        default=["pre-commit", "pre-push"],
        choices=["pre-commit", "pre-push", "post-merge", "post-checkout", "post-rewrite"],
        help="Types of Git hooks to install"
    )
    setup_parser.add_argument(
        "--husky", 
        action="store_true", 
        help="Use Husky for Git hooks"
    )
    setup_parser.add_argument(
        "--remove", 
        action="store_true", 
        help="Remove Git hooks instead of installing them"
    )
    setup_parser.add_argument(
        "--list", 
        action="store_true", 
        help="List installed Git hooks"
    )
    
    # Template management command
    template_parser = subparsers.add_parser("template", help="Manage commit templates")
    template_parser.add_argument(
        "--list", 
        action="store_true", 
        help="List available templates"
    )
    template_parser.add_argument(
        "--add", 
        action="store_true", 
        help="Add a new template"
    )
    template_parser.add_argument(
        "--edit", 
        type=str, 
        help="Edit an existing template"
    )
    template_parser.add_argument(
        "--remove", 
        type=str, 
        help="Remove a template"
    )
    template_parser.add_argument(
        "--set-active", 
        type=str, 
        help="Set the active template"
    )
    template_parser.add_argument(
        "--name", 
        type=str, 
        help="Template name"
    )
    template_parser.add_argument(
        "--subject-template", 
        type=str, 
        help="Subject line template"
    )
    template_parser.add_argument(
        "--body-template", 
        type=str, 
        help="Body template"
    )
    template_parser.add_argument(
        "--footer-template", 
        type=str, 
        help="Footer template"
    )
    
    return parser.parse_args()


def main():
    """Run the Smart Git Commit workflow."""
    args = parse_args()
    
    # Set logging level based on verbose flag
    if args.verbose:
        logger.setLevel(logging.DEBUG)
        logger.debug("Verbose mode enabled - showing debug messages")
    
    # Show version and exit if requested
    if args.version:
        version = get_version()
        print(f"Smart Git Commit v{version}")
        print("\nThis tool is maintained by community contributions.")
        print("If you find it useful, please consider supporting:")
        print("- GitHub Sponsors: https://github.com/sponsors/CripterHack")
        print("- PayPal: http://paypal.com/paypalme/cripterhack")
        return 0
    
    # Launch TUI if requested
    if args.tui:
        try:
            from .tui import run_tui
            run_tui()
            return 0
        except ImportError:
            print("TUI dependencies not installed. Please install textual package.")
            return 1
    
    # Check for subcommands
    if args.command == "squash":
        # Run commit squashing
        from .squash import run_squash_command, SquashStrategy
        
        # Convert string strategy to enum if needed
        strategy = args.strategy
        if strategy != "auto" and hasattr(SquashStrategy, strategy.upper()):
            strategy = getattr(SquashStrategy, strategy.upper())
        
        # Create a workflow object for AI-powered analysis if needed
        workflow = None
        if not args.no_ai:
            workflow = SmartGitCommitWorkflow(
                repo_path=args.repo_path,
                ollama_host=args.ollama_host,
                ollama_model=args.ollama_model,
                use_ai=True,
                timeout=args.timeout,
                security_scan=not args.no_security,
                ai_provider=args.ai_provider,
                openai_api_key=args.openai_api_key,
                openai_model=args.openai_model
            )
        
        success = run_squash_command(
            repo_path=args.repo_path,
            strategy=strategy,
            interactive=not args.no_interactive,
            count=args.count,
            similarity_threshold=args.similarity_threshold,
            time_window_minutes=args.time_window,
            workflow=workflow,
            auto_squash=args.auto_squash
        )
        return 0 if success else 1
    
    elif args.command == "hook":
        # Import hooks module only when needed
        from .hooks import run_hook
        return run_hook(args.hook_type, args.args)
    
    elif args.command == "setup-hooks":
        # Import hooks module only when needed
        if args.list:
            from .hooks import list_installed_hooks
            hooks = list_installed_hooks()
            if hooks:
                print("Installed hooks:")
                for hook, hook_type in hooks.items():
                    print(f"  - {hook} ({hook_type})")
            else:
                print("No Smart Git Commit hooks installed.")
            return 0
            
        if args.remove:
            if args.husky:
                from .hooks import remove_husky
                success = remove_husky()
                if success:
                    print("Husky hooks removed successfully.")
                    return 0
                else:
                    print("Failed to remove Husky hooks.")
                    return 1
            else:
                from .hooks import remove_git_hooks
                results = remove_git_hooks(hooks=args.hook_types)
                success = all(results.values())
                if success:
                    print("Git hooks removed successfully.")
                    return 0
                else:
                    print("Failed to remove some Git hooks:")
                    for hook, result in results.items():
                        print(f"  - {hook}: {'Success' if result else 'Failed'}")
                    return 1
        else:
            if args.husky:
                from .hooks import install_husky
                success = install_husky()
                if success:
                    print("Husky hooks installed successfully.")
                    return 0
                else:
                    print("Failed to install Husky hooks.")
                    return 1
            else:
                from .hooks import install_git_hooks
                from . import __version__
                results = install_git_hooks(hooks=args.hook_types, version=__version__)
                success = all(results.values())
                if success:
                    print("Git hooks installed successfully.")
                    return 0
                else:
                    print("Failed to install some Git hooks:")
                    for hook, result in results.items():
                        print(f"  - {hook}: {'Success' if result else 'Failed'}")
                    return 1
    
    # Handle template command
    elif args.command == "template":
        try:
            from .config import get_config
            config = get_config()
            
            # List templates
            if args.list:
                templates = config.get_commit_templates()
                active = config.get("active_template", "default")
                print_section_header("COMMIT TEMPLATES", not args.no_color)
                for name, template in templates.items():
                    is_active = " (active)" if name == active else ""
                    print(f"\n{name}{is_active}:")
                    print(f"  Subject: {template.get('subject_template', '')}")
                    print(f"  Body: {template.get('body_template', '')}")
                    print(f"  Footer: {template.get('footer_template', '')}")
                return 0
            
            # Add or edit template
            if args.add or args.edit:
                name = args.name or args.edit
                if not name:
                    print("Error: Template name required")
                    return 1
                
                if args.edit and name not in config.get_commit_templates():
                    print(f"Error: Template '{name}' does not exist")
                    return 1
                
                # Get current values for editing
                template = config.get_commit_template(name) if args.edit else {}
                
                subject = args.subject_template or template.get("subject_template", "")
                body = args.body_template or template.get("body_template", "")
                footer = args.footer_template or template.get("footer_template", "")
                
                # Interactive mode if no templates provided
                if not args.subject_template and not args.edit:
                    print(f"Creating template '{name}'")
                    subject = input("Subject template [default: {type}({scope}): {description}]: ") or "{type}({scope}): {description}"
                    body = input("Body template [default: {body}\\n\\nAffected files:\\n{files}]: ") or "{body}\n\nAffected files:\n{files}"
                    footer = input("Footer template [default: {issues}]: ") or "{issues}"
                elif not args.subject_template and args.edit:
                    print(f"Editing template '{name}'")
                    subject = input(f"Subject template [{subject}]: ") or subject
                    body = input(f"Body template [{body}]: ") or body
                    footer = input(f"Footer template [{footer}]: ") or footer
                
                # Add the template
                config.add_commit_template(name, subject, body, footer)
                config.save()
                
                print(f"Template '{name}' {'updated' if args.edit else 'created'} successfully")
                return 0
            
            # Remove template
            if args.remove:
                if config.remove_commit_template(args.remove):
                    config.save()
                    print(f"Template '{args.remove}' removed successfully")
                else:
                    print(f"Error: Could not remove template '{args.remove}' (might be the default template)")
                return 0
            
            # Set active template
            if args.set_active:
                if config.set_active_template(args.set_active):
                    config.save()
                    print(f"Template '{args.set_active}' set as active template")
                else:
                    print(f"Error: Template '{args.set_active}' does not exist")
                return 0
            
            # If no specific action was taken, print usage
            print("Error: No action specified")
            print("Use --list, --add, --edit, --remove, or --set-active")
            return 1
            
        except Exception as e:
            logger.error(f"Error managing templates: {str(e)}")
            if args.verbose:
                import traceback
                traceback.print_exc()
            return 1
    
    # Set up colored output
    use_color = supports_color() and not args.no_color
    
    # Set color theme
    if use_color:
        Colors.set_theme(args.theme)
    
    # Run CLI welcome wizard if needed (first-time setup)
    # Import configuration module here to avoid circular imports
    try:
        from .config import Configuration
        config = Configuration()
        # Run wizard if welcome not completed and not in non-interactive mode
        if not config.get("welcome_completed", False) and not args.non_interactive:
            logger.info("Running first-time CLI welcome wizard")
            try:
                run_cli_welcome_wizard(config, use_color)
                # Config may have changed, refresh
                config = Configuration()
                # Also refresh theme from config if set
                theme_from_config = config.get("theme")
                if theme_from_config and use_color and not args.theme:
                    Colors.set_theme(theme_from_config)
            except KeyboardInterrupt:
                print(f"\n{Colors.get_warning()}First-time setup cancelled. You can run it again next time.{Colors.RESET}")
            except Exception as e:
                logger.error(f"Error during welcome wizard: {e}")
                print(f"{Colors.get_error()}Error during setup: {str(e)}{Colors.RESET}")
    except ImportError:
        logger.warning("Could not import Configuration module, skipping welcome wizard")
    
    # Display banner
    display_banner(use_color)
    
    # Early git status check to avoid unnecessary processing
    try:
        print_section_header("Checking Repository Status", use_color)
        # Simple check if git is installed and repository exists
        result = subprocess.run(
            ["git", "status", "--porcelain"],
            cwd=args.repo_path or ".",
            capture_output=True,
            text=True,
            encoding='utf-8',
            errors='replace'
        )
        
        if result.returncode != 0:
            print(f"\n{Colors.get_error()}âŒ Error: Not a git repository or git command failed.{Colors.RESET}")
            print(f"Please ensure you're in a git repository and git is installed.")
            return 1
            
        # Check if there are any changes to commit
        if not result.stdout.strip():
            print(f"\n{Colors.get_success()}âœ… No changes to commit. Working directory is clean.{Colors.RESET}")
            
            # Thank you message with donation links even when there are no changes
            print("\n" + "-" * 60)
            print(f"{Colors.BOLD}Thank you for using Smart Git Commit! If this tool is helpful,{Colors.RESET}")
            print("please consider supporting development:")
            print(f"{Colors.RED}â¤ï¸  https://github.com/sponsors/CripterHack{Colors.RESET}")
            print(f"{Colors.BLUE}ðŸ’° http://paypal.com/paypalme/cripterhack{Colors.RESET}")
            print("-" * 60 + "\n")
            
            return 0
            
        # Proceed only if there are changes
        change_count = len([line for line in result.stdout.splitlines() if line.strip()])
        print(f"{Colors.get_success()}âœ… Found {change_count} changed files in repository.{Colors.RESET}")
        logger.debug(f"Changes detected in repository ({change_count} files). Proceeding with analysis.")
        
    except Exception as e:
        print(f"\n{Colors.get_error()}âŒ Error checking git status: {str(e)}{Colors.RESET}")
        return 1
    
    # Print welcome message
    print(f"\n{'=' * 60}")
    print(f"{Colors.BOLD}{Colors.BLUE}ðŸš€ Smart Git Commit v{get_version()}{Colors.RESET}")
    print(f"{'=' * 60}")
    print(f"\n{Colors.BOLD}IF THIS TOOL HELPS YOU, PLEASE CONSIDER SUPPORTING IT:{Colors.RESET}")
    print(f"{Colors.RED}â¤ï¸  GitHub Sponsors: https://github.com/sponsors/CripterHack{Colors.RESET}")
    print(f"{Colors.BLUE}ðŸ’° PayPal: http://paypal.com/paypalme/cripterhack{Colors.RESET}")
    print(f"{'=' * 60}\n")
    
    workflow = None
    try:
        # Create the workflow
        print_section_header("Initializing Smart Git Commit", use_color)
        
        workflow = SmartGitCommitWorkflow(
            repo_path=args.repo_path or ".",
            ollama_host=args.ollama_host,
            ollama_model=args.ollama_model,
            use_ai=not args.no_ai,
            timeout=args.timeout,
            skip_hooks=args.skip_hooks,
            parallel=not args.no_parallel,
            security_scan=not args.no_security,
            ai_provider=args.ai_provider,
            openai_api_key=args.openai_api_key,
            openai_model=args.openai_model
        )
        
        # Load changes
        print_section_header("Analyzing Changes", use_color)
        workflow.load_changes()
        
        # Check if there are changes to commit
        if not workflow.changes:
            logger.info("No changes to commit")
            print(f"\n{Colors.get_success()}âœ… No changes to commit. Working directory is clean.{Colors.RESET}")
            
            # Thank you message with donation links even when there are no changes
            print("\n" + "-" * 60)
            print(f"{Colors.BOLD}Thank you for using Smart Git Commit! If this tool is helpful,{Colors.RESET}")
            print("please consider supporting development:")
            print(f"{Colors.RED}â¤ï¸  https://github.com/sponsors/CripterHack{Colors.RESET}")
            print(f"{Colors.BLUE}ðŸ’° http://paypal.com/paypalme/cripterhack{Colors.RESET}")
            print("-" * 60 + "\n")
            
            return 0
            
        # Analyze and group changes
        print(f"{Colors.get_accent()}ðŸ§© Found {len(workflow.changes)} changed files. Analyzing and grouping...{Colors.RESET}")
        
        with Spinner(message="Organizing changes into logical commits...", spinner_type=2):
            workflow.analyze_and_group_changes()
        
        # If analyze-only flag is set, just display the results
        if args.analyze_only:
            print_section_header("Analysis Complete", use_color)
            logger.info(f"Identified {len(workflow.commit_groups)} potential commits")
            for i, group in enumerate(workflow.commit_groups, 1):
                print(f"\nGroup {i}: {group.name} ({group.commit_type.value})")
                workflow._print_files_in_group(group)
            
            # Thank you message with donation links
            print("\n" + "-" * 60)
            print(f"{Colors.BOLD}Thank you for using Smart Git Commit! If this tool is helpful,{Colors.RESET}")
            print("please consider supporting development:")
            print(f"{Colors.RED}â¤ï¸  https://github.com/sponsors/CripterHack{Colors.RESET}")
            print(f"{Colors.BLUE}ðŸ’° http://paypal.com/paypalme/cripterhack{Colors.RESET}")
            print("-" * 60 + "\n")
            
            return 0
        
        print_section_header("Commit Groups", use_color)
        for i, group in enumerate(workflow.commit_groups):
            print(f"  {i+1}. {Colors.get_success()}{group.name}{Colors.RESET} ({group.file_count} files)")
        
        # Execute commits
        print_section_header("Executing Commits", use_color)
        workflow.execute_commits(interactive=not args.non_interactive)
        
        print(f"\n{Colors.get_success()}âœ… Commit operation completed successfully.{Colors.RESET}")
        
        # Thank you message with donation links
        print_section_header("Thank You", use_color)
        print(f"If Smart Git Commit saved you time, please consider supporting development:")
        print(f"{Colors.get_secondary()}â¤ï¸  https://github.com/sponsors/CripterHack{Colors.RESET}")
        print(f"{Colors.get_primary()}ðŸ’° http://paypal.com/paypalme/cripterhack{Colors.RESET}")
        
        return 0
        
    except KeyboardInterrupt:
        print(f"\n\n{Colors.get_warning()}ðŸ›‘ Operation cancelled by user.{Colors.RESET}")
        if workflow and not args.no_revert:
            workflow._revert_staged_changes()
            print(f"{Colors.get_primary()}ðŸ”„ Staged changes have been reverted.{Colors.RESET}")
            
        # Still show donation links on keyboard interrupt
        print("\n" + "-" * 60)
        print(f"{Colors.BOLD}Thanks for trying Smart Git Commit! If you find it useful,{Colors.RESET}")
        print("please consider supporting development:")
        print(f"{Colors.RED}â¤ï¸  https://github.com/sponsors/CripterHack{Colors.RESET}")
        print(f"{Colors.BLUE}ðŸ’° http://paypal.com/paypalme/cripterhack{Colors.RESET}")
        print("-" * 60 + "\n")
        
        return 130  # Standard exit code for SIGINT
        
    except Exception as e:
        logger.error(f"Unexpected error during git commit workflow: {str(e)}", exc_info=True)
        print(f"\n{Colors.get_error()}âŒ UNEXPECTED ERROR: {str(e)}{Colors.RESET}")
        print("\nPlease report this issue with the error details from the log.")
        
        # Revert staged changes if workflow was created
        if workflow and not args.no_revert:
            workflow._revert_staged_changes()
            print(f"{Colors.get_primary()}ðŸ”„ Staged changes have been reverted.{Colors.RESET}")
        
        return 1

if __name__ == "__main__":
    sys.exit(main()) 